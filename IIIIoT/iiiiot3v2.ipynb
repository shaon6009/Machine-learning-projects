{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11109375,"sourceType":"datasetVersion","datasetId":6926121},{"sourceId":11120673,"sourceType":"datasetVersion","datasetId":6934681},{"sourceId":11131669,"sourceType":"datasetVersion","datasetId":6942599}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport plotly.express as px\n%matplotlib inline\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n# from cuml.preprocessing import StandardScaler\n\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n\n# import cudf # insted of pandas so it use gpu\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:10:10.266384Z","iopub.execute_input":"2025-04-27T14:10:10.266754Z","iopub.status.idle":"2025-04-27T14:10:10.277602Z","shell.execute_reply.started":"2025-04-27T14:10:10.266725Z","shell.execute_reply":"2025-04-27T14:10:10.276138Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"data= pd.read_csv('/kaggle/input/afterlightgbm/lightGBMplustarget.csv')\ndata\n\n\n\nX = data.drop(columns=['Attack_Num'])\ny = data['Attack_Num']\n\nX.replace([np.inf, -np.inf], np.nan, inplace=True)  \nX.fillna(X.mean(), inplace=True)\n\nX.fillna(X.mean(), inplace=True)\n\nX.fillna(X.mean(), inplace=True)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_test.shape) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:10:14.588871Z","iopub.execute_input":"2025-04-27T14:10:14.589222Z","iopub.status.idle":"2025-04-27T14:11:01.927617Z","shell.execute_reply.started":"2025-04-27T14:10:14.589192Z","shell.execute_reply":"2025-04-27T14:11:01.926624Z"}},"outputs":[{"name":"stdout","text":"(15246446, 24) (1694050, 24)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n\n# def remove_outliers_iqr(df):\n#     for col in df.select_dtypes(include=['int64', 'float64']).columns:\n#         Q1 = df[col].quantile(0.05)\n#         Q3 = df[col].quantile(0.95)\n#         IQR = Q3 - Q1\n#         lower_bound = Q1 - 1.6 * IQR\n#         upper_bound = Q3 + 1.6 * IQR\n#         df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n    \n#     return df\n\n# cleaned_data = remove_outliers_iqr(data)\n\n# scaler = StandardScaler()\n# scaled_data = scaler.fit_transform(cleaned_data)\n\n# scaled_df = pd.DataFrame(scaled_data, columns=cleaned_data.columns)\n\n# print(scaled_df.head())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# num_unique_values = scaled_df['Attack_Num'].nunique()\n# unique_values = scaled_df['Attack_Num'].unique()\n\n# print(f\"Attack_Num has {num_unique_values} unique values.\")\n# print(\"Unique values:\", unique_values)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(scaled_df['Attack_Num'].value_counts()) \n\n# attack_mapping = {\n#     '-0.84968125': 0,\n#     '4.5686718': 1,\n#     '0.35439721': 2,\n#     '-0.24764202': 3,\n#     '1.55847566': 4,\n#     '2.16051489': 5,\n#     '0.95643643': 6,\n#     '2.76255412': 7,\n#     '3.96663257': 8,\n#     '3.36459334': 9\n# }\n\n# data['Attack_Num'] = data['Attack_Num'].map(attack_mapping)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X = scaled_df.drop(columns=['Attack_Num'])\n# y = scaled_df['Attack_Num'].astype(int)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unique_classes = y.nunique()\n# print(f\"Number of unique classes in Attack_Num: {unique_classes}\")\n# print(f\"Unique values in Attack_Num: {y.unique()}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data.drop(columns=['Attack_Num'])\ny = data['Attack_Num']\n\n# X = data.drop(columns=['Attack_Num','DST_TO_SRC_SECOND_BYTES','DURATION_IN','MIN_TTL','DNS_TTL_ANSWER','MIN_IP_PKT_LEN'])\n# y = data['Attack_Num']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:06:11.578946Z","iopub.execute_input":"2025-04-27T14:06:11.579269Z","iopub.status.idle":"2025-04-27T14:06:12.515334Z","shell.execute_reply.started":"2025-04-27T14:06:11.579236Z","shell.execute_reply":"2025-04-27T14:06:12.514505Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# --------------","metadata":{}},{"cell_type":"code","source":"# print(np.isinf(X).sum().sum())  \n# print(np.isinf(y).sum()) \n# print(X.describe())  ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.replace([np.inf, -np.inf], np.nan, inplace=True)  \nX.fillna(X.mean(), inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:06:12.516579Z","iopub.execute_input":"2025-04-27T14:06:12.516853Z","iopub.status.idle":"2025-04-27T14:06:14.572520Z","shell.execute_reply.started":"2025-04-27T14:06:12.516830Z","shell.execute_reply":"2025-04-27T14:06:14.571497Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# print(X.isna().sum().sum()) ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.fillna(X.mean(), inplace=True)\n# print(X.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:06:14.573538Z","iopub.execute_input":"2025-04-27T14:06:14.573896Z","iopub.status.idle":"2025-04-27T14:06:16.466708Z","shell.execute_reply.started":"2025-04-27T14:06:14.573860Z","shell.execute_reply":"2025-04-27T14:06:16.465693Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:06:16.467618Z","iopub.execute_input":"2025-04-27T14:06:16.467981Z","iopub.status.idle":"2025-04-27T14:06:23.940490Z","shell.execute_reply.started":"2025-04-27T14:06:16.467946Z","shell.execute_reply":"2025-04-27T14:06:23.939488Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: overflow encountered in square\n  new_unnormalized_variance -= correction**2 / new_sample_count\n/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in subtract\n  new_unnormalized_variance -= correction**2 / new_sample_count\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:86: RuntimeWarning: overflow encountered in square\n  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:87: RuntimeWarning: invalid value encountered in less_equal\n  return var <= upper_bound\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# y.nunique()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_test.shape) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:06:23.942203Z","iopub.execute_input":"2025-04-27T14:06:23.942525Z","iopub.status.idle":"2025-04-27T14:06:28.623692Z","shell.execute_reply.started":"2025-04-27T14:06:23.942492Z","shell.execute_reply":"2025-04-27T14:06:28.622662Z"}},"outputs":[{"name":"stdout","text":"(15246446, 24) (1694050, 24)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier(n_estimators=100, max_depth=30, n_jobs=-1, random_state=42)\nmodel.fit(X_train, y_train)\n\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nclassification_rep = classification_report(y_test, y_test_pred)\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\nprint(f\"Training Accuracy: {train_accuracy:.4f}\")\nprint(f\"Testing Accuracy: {test_accuracy:.4f}\")\nprint(\"\\nClassification Report:\\n\", classification_rep)\nprint(\"\\nConfusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:11:01.928803Z","iopub.execute_input":"2025-04-27T14:11:01.929067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n\nclf = AdaBoostClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"AdaBoostClassifier Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nclf = LGBMClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"LightGBM Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.svm import SVC\n\n# clf = SVC(kernel='rbf', probability=True)\n# clf.fit(X_train, y_train)\n# y_pred = clf.predict(X_test)\n\n# print(\"SVM (SVC) Results:\")\n# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n# print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n# print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n# print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nclf = XGBClassifier(n_estimators=100, learning_rate=0.1, n_jobs=-1, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"XGBoost Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"GradientBoosting Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1 no","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\ninputs = Input(shape=(X.shape[1],))\n\nweights = Dense(X.shape[1], activation='sigmoid', use_bias=False)(inputs)\nweighted = Multiply()([inputs, weights])\n\nx = BatchNormalization()(weighted)\nx = Dense(128, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(64, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(32, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\n\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=3,\n                    batch_size=256,\n                    callbacks=[early_stop, reduce_lr])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 no","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\n\n\n# === Dummy Data (Replace with your real data X, y) ===\n# Example: X = np.random.rand(1000, 20), y = np.random.randint(0, 10, 1000)\n# Ensure that X and y are loaded or generated before proceeding\n\n# # Standardize features\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X)\n\n# # Train-validation split\n# X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# # === Residual Block Function ===\ndef residual_block(x, units):\n    shortcut = x\n    if x.shape[-1] != units:\n        shortcut = Dense(units, kernel_initializer='glorot_normal')(shortcut)\n\n    x = Dense(units, kernel_initializer='glorot_normal',\n              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n    x = LeakyReLU()(x)\n    x = BatchNormalization()(x)\n    x = Dense(units, kernel_initializer='glorot_normal',\n              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n\n    x = Add()([x, shortcut])\n    x = LeakyReLU()(x)\n    x = BatchNormalization()(x)\n    return x\n\n# === Model Architecture ===\ninputs = Input(shape=(X.shape[1],))\n\nweights = Dense(X.shape[1], activation='sigmoid', use_bias=False)(inputs)\nweighted = Multiply()([inputs, weights])\n\nx = BatchNormalization()(weighted)\nx = Dense(128, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.4)(x)\n\nx = residual_block(x, 64)\n\nx = Dense(32, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\nx = Dropout(0.3)(x)\n\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# === Try label smoothing if available ===\ntry:\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1)\nexcept TypeError:\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.009),\n              loss=loss_fn,\n              metrics=['accuracy'])\n\n# === Callbacks ===\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n\n# === Train Model ===\nhistory = model.fit(X_train, y_train,\n                    validation_data=(X_val, y_val),\n                    epochs=15,\n                    batch_size=256,\n                    callbacks=[early_stop, reduce_lr])\n\n# === Evaluate ===\nval_loss, val_acc = model.evaluate(X_val, y_val)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:50:34.202042Z","iopub.execute_input":"2025-04-14T02:50:34.202403Z","iopub.status.idle":"2025-04-14T03:25:08.384645Z","shell.execute_reply.started":"2025-04-14T02:50:34.202378Z","shell.execute_reply":"2025-04-14T03:25:08.383683Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2ms/step - accuracy: 0.9183 - loss: 0.2672 - val_accuracy: 0.9525 - val_loss: 0.1663 - learning_rate: 0.0090\nEpoch 2/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - accuracy: 0.9458 - loss: 0.1925 - val_accuracy: 0.9451 - val_loss: 0.1806 - learning_rate: 0.0090\nEpoch 3/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9484 - loss: 0.1863 - val_accuracy: 0.9552 - val_loss: 11.3956 - learning_rate: 0.0090\nEpoch 4/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - accuracy: 0.9493 - loss: 0.1837 - val_accuracy: 0.9561 - val_loss: 0.1676 - learning_rate: 0.0090\nEpoch 5/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - accuracy: 0.9502 - loss: 0.1817 - val_accuracy: 0.9550 - val_loss: 0.1543 - learning_rate: 0.0090\nEpoch 6/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - accuracy: 0.9507 - loss: 0.1802 - val_accuracy: 0.9565 - val_loss: 5.8885 - learning_rate: 0.0090\nEpoch 7/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9511 - loss: 0.1784 - val_accuracy: 0.9559 - val_loss: 0.1643 - learning_rate: 0.0090\nEpoch 8/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9515 - loss: 0.1771 - val_accuracy: 0.9571 - val_loss: 0.1606 - learning_rate: 0.0090\nEpoch 9/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9518 - loss: 0.1755 - val_accuracy: 0.9575 - val_loss: 0.1560 - learning_rate: 0.0090\nEpoch 10/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - accuracy: 0.9521 - loss: 0.1753 - val_accuracy: 0.9550 - val_loss: 0.1585 - learning_rate: 0.0090\nEpoch 11/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1543 - val_accuracy: 0.9574 - val_loss: 0.1401 - learning_rate: 0.0045\nEpoch 12/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9545 - loss: 0.1545 - val_accuracy: 0.9582 - val_loss: 0.1375 - learning_rate: 0.0045\nEpoch 13/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9547 - loss: 0.1545 - val_accuracy: 0.9597 - val_loss: 0.1338 - learning_rate: 0.0045\nEpoch 14/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1543 - val_accuracy: 0.9596 - val_loss: 0.1361 - learning_rate: 0.0045\nEpoch 15/15\n\u001b[1m59557/59557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1544 - val_accuracy: 0.9599 - val_loss: 0.1361 - learning_rate: 0.0045\n\u001b[1m52940/52940\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1ms/step - accuracy: 0.9601 - loss: 0.1360\nValidation Accuracy: 0.9599\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 3 no","metadata":{}},{"cell_type":"markdown","source":"Attack\n\n0.Benign        6099469\n\n1.scanning      3781419\n\n2.xss           2455020\n\n3.ddos          2026234\n\n4.password      1153323\n\n5.dos            712609\n\n6.injection      684465\n\n4.backdoor        16809\n\n8.mitm             7723\n\n9.ransomware       3425","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport numpy as np\n\ny_train = np.array(y_train).astype(int)  \n\nclass_weights = {\n    0: 1.0,  \n    1: 1.5,   \n    2: 1.0,   \n    3: 1.5,   \n    4: 2.0,   \n    5: 2.8,   \n    6: 3.5,   \n    7: 4.4,   \n    8: 6.7, \n    9: 8.0   \n}\n# class_weights = {\n#     0: 1.0,   # Benign (most common, lowest weight)\n#     1: 1.5,   # Scanning\n#     2: 2.0,   # XSS\n#     3: 2.5,   # DDoS\n#     4: 3.0,   # Password\n#     5: 4.0,   # DoS\n#     6: 4.5,   # Injection\n#     7: 8.0,   # Backdoor\n#     8: 10.0,  # MITM\n#     9: 12.0   # Ransomware (rarest, highest weight)\n# }\n\nclass_weights = {label: class_weights.get(label, 1.0) for label in set(y_train)}\n\ninputs = Input(shape=(X.shape[1],))\n\nfeature_weights = Dense(X.shape[1], activation='sigmoid', use_bias=False)(inputs)\nweighted = Multiply()([inputs, feature_weights])\n\nx = BatchNormalization()(weighted)\nx = Dense(256, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.4)(x)\n\nx = Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(64, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\n\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Optimizer with Weight Decay\noptimizer = tf.keras.optimizers.AdamW(learning_rate=0.004, weight_decay=1e-4)\n\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=7, mode='max', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=3,\n                    batch_size=256,\n                    class_weight=class_weights,\n                    callbacks=[early_stop, reduce_lr])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 no","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)  \nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n\ny_train = np.array(y_train).astype(int)\ny_test = np.array(y_test).astype(int)\n\n# Adjusted Class Weights for Extreme Imbalance\nclass_weights = {\n    0: 1.0,    # Benign (Majority)\n    1: 1.5,    # Scanning\n    2: 2.5,    # XSS\n    3: 3.0,    # DDoS\n    4: 4.0,    # Password\n    5: 5.0,    # DoS\n    6: 6.0,    # Injection\n    7: 20.0,   # Backdoor (Rare)\n    8: 25.0,   # MITM (Extremely Rare)\n    9: 30.0    # Ransomware (Most Rare)\n}\n\nclass_weights = {label: class_weights.get(label, 1.0) for label in np.unique(y_train)}\n\ninputs = Input(shape=(X_train.shape[1],))\n\nx = BatchNormalization()(inputs)\nfeature_weights = Dense(X_train.shape[1], activation='sigmoid', use_bias=False)(x)\nweighted = Multiply()([x, feature_weights])\n\nx = Dense(256, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001))(weighted)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.4)(x)  \n\nx = Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.4)(x)\n\nx = Dense(64, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(32, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\n\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\noptimizer = tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=5e-5)\n\n# **Focal Loss Function** for handling class imbalance better\n# # **Corrected Focal Loss Function**  \n# def focal_loss(alpha=0.25, gamma=2.0):\n#     def loss(y_true, y_pred):\n#         y_true = tf.squeeze(tf.one_hot(tf.cast(y_true, tf.int32), depth=10))  # One-hot conversion\n#         cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n#         weight = alpha * tf.pow((1 - y_pred), gamma)\n#         return tf.reduce_mean(weight * cross_entropy)\n    # return loss\n\n\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',  # Directly works with integer labels\n              metrics=['accuracy'])\n\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=3,  \n                    batch_size=128,  \n                    class_weight=class_weights,\n                    callbacks=[early_stop, reduce_lr],\n                    verbose=1)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5 no","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\ninputs = Input(shape=(X.shape[1],))\n\nfeature_weights = Dense(X.shape[1], activation='sigmoid', use_bias=False)(inputs)\nweighted = Multiply()([inputs, feature_weights])\n\nx = BatchNormalization()(weighted)\nx = Dense(128, kernel_initializer='he_normal')(x)\nx = LeakyReLU()(x)\nx = Dropout(0.5)(x)\n\nx = Dense(64, kernel_initializer='he_normal')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\n\nx = Dense(32, kernel_initializer='he_normal')(x)\nx = LeakyReLU()(x)\n\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\noptimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=3,\n                    batch_size=256, \n                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)])\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7 no","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# **Apply Log Transform to Reduce Skewness**\ndef log_transform(X):\n    return np.log1p(X)  # log(1 + x) to handle zero values\n\n# **Apply MinMax Scaling**\ndef scale_data(X):\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(X)\n\n# **Load Data (Assuming X, y are already defined)**\nX_log = log_transform(X)  # Step 1: Log Transform\nX_scaled = scale_data(X_log)  # Step 2: MinMax Scaling\n\n# **Train-Test Split**\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\ny_train = np.array(y_train)  # Convert Pandas Series to NumPy array\nclass_weights = {\n    0: 1.0,  \n    1: 1.5,   \n    2: 2.0,   \n    3: 2.5,   \n    4: 3.0,   \n    5: 4.0,   \n    6: 5.0,   \n    7: 8.0,   \n    8: 10.0,  \n    9: 12.0   \n}\n\n# Ensure dictionary keys match unique labels in y_train\nunique_labels = np.unique(y_train)\nclass_weights = {label: class_weights.get(label, 1.0) for label in unique_labels}\n\n\n# **Feature Weighting with Correlations**\ncorrelations = np.array([\n    0.066518, 0.055398, 0.301534, -0.328661, -0.120393, 0.406636, \n    -0.000028, 0.303211, 0.118396, 0.090655, -0.023103, 0.123218,\n    0.507476, -0.000000, 0.508130, 0.166764, 0.477539, 0.366213,\n    0.003692, 0.512159, -0.077695, 0.509574, 0.150356, 0.000000\n])\n\n# **Neural Network Architecture**\ninputs = Input(shape=(X_scaled.shape[1],))\n\n# Feature Weighting\nweighted = Multiply()([inputs, tf.constant(correlations.reshape(1, -1), dtype=tf.float32)])\n\n# Deep Feature Extraction\nx = BatchNormalization()(weighted)\nx = Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(64, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\n\nx = Dense(32, kernel_initializer='glorot_normal')(x)\nx = LeakyReLU()(x)\n\n# Output Layer\noutputs = Dense(10, activation='softmax')(x)\n\n# **Compile Model**\nmodel = Model(inputs=inputs, outputs=outputs)\noptimizer = tf.keras.optimizers.AdamW(learning_rate=0.002, weight_decay=1e-4)\n\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# **Callbacks**\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n\n# **Train the Model**\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=3,  # Increased for better learning\n                    batch_size=512,  # Adjusted for large dataset\n                    class_weight=class_weights,\n                    callbacks=[early_stop, reduce_lr])\n\n# **Model Summary**\nmodel.summary()\n\n# **Evaluate Model**\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8 no","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Multiply, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nX = np.log1p(X) \nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\ninputs = Input(shape=(X.shape[1],))\n\nfeature_weights = Dense(X.shape[1], activation='sigmoid', use_bias=False)(inputs)\nweighted = Multiply()([inputs, feature_weights])\n\nx = BatchNormalization()(weighted)\nx = Dense(128, kernel_initializer='he_normal')(x)\nx = LeakyReLU()(x)\nx = Dropout(0.5)(x)\n\nx = Dense(64, kernel_initializer='he_normal')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\n\nx = Dense(32, kernel_initializer='he_normal')(x)\nx = LeakyReLU()(x)\n\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\noptimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2,\n                    epochs=3, \n                    batch_size=128,  \n                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)])\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"✅ Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\npip install --upgrade tensorflow","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-14T00:16:42.375Z"}},"outputs":[],"execution_count":null}]}
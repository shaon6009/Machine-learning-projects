{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":723448,"sourceType":"datasetVersion","datasetId":371805}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport plotly.express as px\n%matplotlib inline\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report)\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier,GradientBoostingClassifier, BaggingClassifier, HistGradientBoostingClassifier, StackingClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import ADASYN\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.base import FunctionSampler\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T20:19:21.067927Z","iopub.execute_input":"2024-10-25T20:19:21.068413Z","iopub.status.idle":"2024-10-25T20:19:21.089276Z","shell.execute_reply.started":"2024-10-25T20:19:21.068370Z","shell.execute_reply":"2024-10-25T20:19:21.087982Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data= pd.read_csv('/kaggle/input/framingham/framingham.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:43.875860Z","iopub.execute_input":"2024-10-25T20:18:43.876645Z","iopub.status.idle":"2024-10-25T20:18:43.952316Z","shell.execute_reply.started":"2024-10-25T20:18:43.876601Z","shell.execute_reply":"2024-10-25T20:18:43.951193Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"         Sex  age  education currentSmoker  cigsPerDay  BPMeds  \\\n0       male   39        4.0            No         0.0     0.0   \n1     female   46        2.0            No         0.0     0.0   \n2       male   48        1.0           Yes        20.0     0.0   \n3     female   61        3.0           Yes        30.0     0.0   \n4     female   46        3.0           Yes        23.0     0.0   \n...      ...  ...        ...           ...         ...     ...   \n4235  female   48        2.0           Yes        20.0     NaN   \n4236  female   44        1.0           Yes        15.0     0.0   \n4237  female   52        2.0            No         0.0     0.0   \n4238    male   40        3.0            No         0.0     0.0   \n4239  female   39        3.0           Yes        30.0     0.0   \n\n      prevalentStroke  prevalentHyp diabetes  totChol  sysBP  diaBP    BMI  \\\n0                   0             0       No    195.0  106.0   70.0  26.97   \n1                   0             0       No    250.0  121.0   81.0  28.73   \n2                   0             0       No    245.0  127.5   80.0  25.34   \n3                   0             1       No    225.0  150.0   95.0  28.58   \n4                   0             0       No    285.0  130.0   84.0  23.10   \n...               ...           ...      ...      ...    ...    ...    ...   \n4235                0             0       No    248.0  131.0   72.0  22.00   \n4236                0             0       No    210.0  126.5   87.0  19.16   \n4237                0             0       No    269.0  133.5   83.0  21.47   \n4238                0             1       No    185.0  141.0   98.0  25.60   \n4239                0             0       No    196.0  133.0   86.0  20.91   \n\n      heartRate  glucose  TenYearCHD  \n0          80.0     77.0           0  \n1          95.0     76.0           0  \n2          75.0     70.0           0  \n3          65.0    103.0           1  \n4          85.0     85.0           0  \n...         ...      ...         ...  \n4235       84.0     86.0           0  \n4236       86.0      NaN           0  \n4237       80.0    107.0           0  \n4238       67.0     72.0           0  \n4239       85.0     80.0           0  \n\n[4240 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n      <th>TenYearCHD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>male</td>\n      <td>39</td>\n      <td>4.0</td>\n      <td>No</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>female</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>No</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>male</td>\n      <td>48</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>female</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>Yes</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>No</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>female</td>\n      <td>46</td>\n      <td>3.0</td>\n      <td>Yes</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4235</th>\n      <td>female</td>\n      <td>48</td>\n      <td>2.0</td>\n      <td>Yes</td>\n      <td>20.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>248.0</td>\n      <td>131.0</td>\n      <td>72.0</td>\n      <td>22.00</td>\n      <td>84.0</td>\n      <td>86.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4236</th>\n      <td>female</td>\n      <td>44</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>210.0</td>\n      <td>126.5</td>\n      <td>87.0</td>\n      <td>19.16</td>\n      <td>86.0</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4237</th>\n      <td>female</td>\n      <td>52</td>\n      <td>2.0</td>\n      <td>No</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>269.0</td>\n      <td>133.5</td>\n      <td>83.0</td>\n      <td>21.47</td>\n      <td>80.0</td>\n      <td>107.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4238</th>\n      <td>male</td>\n      <td>40</td>\n      <td>3.0</td>\n      <td>No</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>No</td>\n      <td>185.0</td>\n      <td>141.0</td>\n      <td>98.0</td>\n      <td>25.60</td>\n      <td>67.0</td>\n      <td>72.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4239</th>\n      <td>female</td>\n      <td>39</td>\n      <td>3.0</td>\n      <td>Yes</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>196.0</td>\n      <td>133.0</td>\n      <td>86.0</td>\n      <td>20.91</td>\n      <td>85.0</td>\n      <td>80.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4240 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:43.954542Z","iopub.execute_input":"2024-10-25T20:18:43.954910Z","iopub.status.idle":"2024-10-25T20:18:43.980541Z","shell.execute_reply.started":"2024-10-25T20:18:43.954865Z","shell.execute_reply":"2024-10-25T20:18:43.979510Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4240 entries, 0 to 4239\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Sex              4240 non-null   object \n 1   age              4240 non-null   int64  \n 2   education        4135 non-null   float64\n 3   currentSmoker    4240 non-null   object \n 4   cigsPerDay       4211 non-null   float64\n 5   BPMeds           4187 non-null   float64\n 6   prevalentStroke  4240 non-null   int64  \n 7   prevalentHyp     4240 non-null   int64  \n 8   diabetes         4240 non-null   object \n 9   totChol          4190 non-null   float64\n 10  sysBP            4240 non-null   float64\n 11  diaBP            4240 non-null   float64\n 12  BMI              4221 non-null   float64\n 13  heartRate        4239 non-null   float64\n 14  glucose          3852 non-null   float64\n 15  TenYearCHD       4240 non-null   int64  \ndtypes: float64(9), int64(4), object(3)\nmemory usage: 530.1+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"round(data.describe().T,2)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:43.981712Z","iopub.execute_input":"2024-10-25T20:18:43.982061Z","iopub.status.idle":"2024-10-25T20:18:44.039387Z","shell.execute_reply.started":"2024-10-25T20:18:43.982026Z","shell.execute_reply":"2024-10-25T20:18:44.038314Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                  count    mean    std     min     25%    50%     75%    max\nage              4240.0   49.58   8.57   32.00   42.00   49.0   56.00   70.0\neducation        4135.0    1.98   1.02    1.00    1.00    2.0    3.00    4.0\ncigsPerDay       4211.0    9.01  11.92    0.00    0.00    0.0   20.00   70.0\nBPMeds           4187.0    0.03   0.17    0.00    0.00    0.0    0.00    1.0\nprevalentStroke  4240.0    0.01   0.08    0.00    0.00    0.0    0.00    1.0\nprevalentHyp     4240.0    0.31   0.46    0.00    0.00    0.0    1.00    1.0\ntotChol          4190.0  236.70  44.59  107.00  206.00  234.0  263.00  696.0\nsysBP            4240.0  132.35  22.03   83.50  117.00  128.0  144.00  295.0\ndiaBP            4240.0   82.90  11.91   48.00   75.00   82.0   90.00  142.5\nBMI              4221.0   25.80   4.08   15.54   23.07   25.4   28.04   56.8\nheartRate        4239.0   75.88  12.03   44.00   68.00   75.0   83.00  143.0\nglucose          3852.0   81.96  23.95   40.00   71.00   78.0   87.00  394.0\nTenYearCHD       4240.0    0.15   0.36    0.00    0.00    0.0    0.00    1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>age</th>\n      <td>4240.0</td>\n      <td>49.58</td>\n      <td>8.57</td>\n      <td>32.00</td>\n      <td>42.00</td>\n      <td>49.0</td>\n      <td>56.00</td>\n      <td>70.0</td>\n    </tr>\n    <tr>\n      <th>education</th>\n      <td>4135.0</td>\n      <td>1.98</td>\n      <td>1.02</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>2.0</td>\n      <td>3.00</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>cigsPerDay</th>\n      <td>4211.0</td>\n      <td>9.01</td>\n      <td>11.92</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>20.00</td>\n      <td>70.0</td>\n    </tr>\n    <tr>\n      <th>BPMeds</th>\n      <td>4187.0</td>\n      <td>0.03</td>\n      <td>0.17</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>prevalentStroke</th>\n      <td>4240.0</td>\n      <td>0.01</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>prevalentHyp</th>\n      <td>4240.0</td>\n      <td>0.31</td>\n      <td>0.46</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>totChol</th>\n      <td>4190.0</td>\n      <td>236.70</td>\n      <td>44.59</td>\n      <td>107.00</td>\n      <td>206.00</td>\n      <td>234.0</td>\n      <td>263.00</td>\n      <td>696.0</td>\n    </tr>\n    <tr>\n      <th>sysBP</th>\n      <td>4240.0</td>\n      <td>132.35</td>\n      <td>22.03</td>\n      <td>83.50</td>\n      <td>117.00</td>\n      <td>128.0</td>\n      <td>144.00</td>\n      <td>295.0</td>\n    </tr>\n    <tr>\n      <th>diaBP</th>\n      <td>4240.0</td>\n      <td>82.90</td>\n      <td>11.91</td>\n      <td>48.00</td>\n      <td>75.00</td>\n      <td>82.0</td>\n      <td>90.00</td>\n      <td>142.5</td>\n    </tr>\n    <tr>\n      <th>BMI</th>\n      <td>4221.0</td>\n      <td>25.80</td>\n      <td>4.08</td>\n      <td>15.54</td>\n      <td>23.07</td>\n      <td>25.4</td>\n      <td>28.04</td>\n      <td>56.8</td>\n    </tr>\n    <tr>\n      <th>heartRate</th>\n      <td>4239.0</td>\n      <td>75.88</td>\n      <td>12.03</td>\n      <td>44.00</td>\n      <td>68.00</td>\n      <td>75.0</td>\n      <td>83.00</td>\n      <td>143.0</td>\n    </tr>\n    <tr>\n      <th>glucose</th>\n      <td>3852.0</td>\n      <td>81.96</td>\n      <td>23.95</td>\n      <td>40.00</td>\n      <td>71.00</td>\n      <td>78.0</td>\n      <td>87.00</td>\n      <td>394.0</td>\n    </tr>\n    <tr>\n      <th>TenYearCHD</th>\n      <td>4240.0</td>\n      <td>0.15</td>\n      <td>0.36</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.041244Z","iopub.execute_input":"2024-10-25T20:18:44.041626Z","iopub.status.idle":"2024-10-25T20:18:44.055187Z","shell.execute_reply.started":"2024-10-25T20:18:44.041587Z","shell.execute_reply":"2024-10-25T20:18:44.053941Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# from ydata_profiling import ProfileReport\n# profile=ProfileReport(data,explorative=True,dark_mode=True)\n# profile","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.056477Z","iopub.execute_input":"2024-10-25T20:18:44.056922Z","iopub.status.idle":"2024-10-25T20:18:44.062262Z","shell.execute_reply.started":"2024-10-25T20:18:44.056869Z","shell.execute_reply":"2024-10-25T20:18:44.061126Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.063768Z","iopub.execute_input":"2024-10-25T20:18:44.064262Z","iopub.status.idle":"2024-10-25T20:18:44.079256Z","shell.execute_reply.started":"2024-10-25T20:18:44.064209Z","shell.execute_reply":"2024-10-25T20:18:44.077944Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Sex', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n       'diaBP', 'BMI', 'heartRate', 'glucose', 'TenYearCHD'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"unique_values= {}\nfor i in data.columns:\n    unique_values[i]= data[i].nunique()\n    \npd.DataFrame(unique_values,index= ['unique_value']).T","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.083824Z","iopub.execute_input":"2024-10-25T20:18:44.084416Z","iopub.status.idle":"2024-10-25T20:18:44.104413Z","shell.execute_reply.started":"2024-10-25T20:18:44.084373Z","shell.execute_reply":"2024-10-25T20:18:44.103313Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                 unique_value\nSex                         2\nage                        39\neducation                   4\ncurrentSmoker               2\ncigsPerDay                 33\nBPMeds                      2\nprevalentStroke             2\nprevalentHyp                2\ndiabetes                    2\ntotChol                   248\nsysBP                     234\ndiaBP                     146\nBMI                      1364\nheartRate                  73\nglucose                   143\nTenYearCHD                  2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sex</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>education</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>currentSmoker</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>cigsPerDay</th>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>BPMeds</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>prevalentStroke</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>prevalentHyp</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>diabetes</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>totChol</th>\n      <td>248</td>\n    </tr>\n    <tr>\n      <th>sysBP</th>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>diaBP</th>\n      <td>146</td>\n    </tr>\n    <tr>\n      <th>BMI</th>\n      <td>1364</td>\n    </tr>\n    <tr>\n      <th>heartRate</th>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>glucose</th>\n      <td>143</td>\n    </tr>\n    <tr>\n      <th>TenYearCHD</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.105821Z","iopub.execute_input":"2024-10-25T20:18:44.106249Z","iopub.status.idle":"2024-10-25T20:18:44.117895Z","shell.execute_reply.started":"2024-10-25T20:18:44.106208Z","shell.execute_reply":"2024-10-25T20:18:44.116713Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Sex                  0\nage                  0\neducation          105\ncurrentSmoker        0\ncigsPerDay          29\nBPMeds              53\nprevalentStroke      0\nprevalentHyp         0\ndiabetes             0\ntotChol             50\nsysBP                0\ndiaBP                0\nBMI                 19\nheartRate            1\nglucose            388\nTenYearCHD           0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data['Sex'] = data['Sex'].map({'male': 1, 'female': 0})\ndata['currentSmoker']= data['currentSmoker'].map({'Yes':1, 'No':0})\ndata['diabetes']= data['diabetes'].map({'Yes':1, 'No':0})\ndata","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.119288Z","iopub.execute_input":"2024-10-25T20:18:44.119631Z","iopub.status.idle":"2024-10-25T20:18:44.160951Z","shell.execute_reply.started":"2024-10-25T20:18:44.119596Z","shell.execute_reply":"2024-10-25T20:18:44.159807Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"      Sex  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n0       1   39        4.0              0         0.0     0.0                0   \n1       0   46        2.0              0         0.0     0.0                0   \n2       1   48        1.0              1        20.0     0.0                0   \n3       0   61        3.0              1        30.0     0.0                0   \n4       0   46        3.0              1        23.0     0.0                0   \n...   ...  ...        ...            ...         ...     ...              ...   \n4235    0   48        2.0              1        20.0     NaN                0   \n4236    0   44        1.0              1        15.0     0.0                0   \n4237    0   52        2.0              0         0.0     0.0                0   \n4238    1   40        3.0              0         0.0     0.0                0   \n4239    0   39        3.0              1        30.0     0.0                0   \n\n      prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  \\\n0                0         0    195.0  106.0   70.0  26.97       80.0   \n1                0         0    250.0  121.0   81.0  28.73       95.0   \n2                0         0    245.0  127.5   80.0  25.34       75.0   \n3                1         0    225.0  150.0   95.0  28.58       65.0   \n4                0         0    285.0  130.0   84.0  23.10       85.0   \n...            ...       ...      ...    ...    ...    ...        ...   \n4235             0         0    248.0  131.0   72.0  22.00       84.0   \n4236             0         0    210.0  126.5   87.0  19.16       86.0   \n4237             0         0    269.0  133.5   83.0  21.47       80.0   \n4238             1         0    185.0  141.0   98.0  25.60       67.0   \n4239             0         0    196.0  133.0   86.0  20.91       85.0   \n\n      glucose  TenYearCHD  \n0        77.0           0  \n1        76.0           0  \n2        70.0           0  \n3       103.0           1  \n4        85.0           0  \n...       ...         ...  \n4235     86.0           0  \n4236      NaN           0  \n4237    107.0           0  \n4238     72.0           0  \n4239     80.0           0  \n\n[4240 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n      <th>TenYearCHD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>39</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>48</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>46</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4235</th>\n      <td>0</td>\n      <td>48</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248.0</td>\n      <td>131.0</td>\n      <td>72.0</td>\n      <td>22.00</td>\n      <td>84.0</td>\n      <td>86.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4236</th>\n      <td>0</td>\n      <td>44</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>210.0</td>\n      <td>126.5</td>\n      <td>87.0</td>\n      <td>19.16</td>\n      <td>86.0</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4237</th>\n      <td>0</td>\n      <td>52</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>269.0</td>\n      <td>133.5</td>\n      <td>83.0</td>\n      <td>21.47</td>\n      <td>80.0</td>\n      <td>107.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4238</th>\n      <td>1</td>\n      <td>40</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>185.0</td>\n      <td>141.0</td>\n      <td>98.0</td>\n      <td>25.60</td>\n      <td>67.0</td>\n      <td>72.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4239</th>\n      <td>0</td>\n      <td>39</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>196.0</td>\n      <td>133.0</td>\n      <td>86.0</td>\n      <td>20.91</td>\n      <td>85.0</td>\n      <td>80.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4240 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"target = data['TenYearCHD']\ncolumns_to_drop = ['TenYearCHD']\nfeatures = data.drop(columns_to_drop, axis=1)\n\nX = features.values\ny = target.values\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#xx_train, xx_test, yy_train, yy_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.162383Z","iopub.execute_input":"2024-10-25T20:18:44.162804Z","iopub.status.idle":"2024-10-25T20:18:44.174984Z","shell.execute_reply.started":"2024-10-25T20:18:44.162765Z","shell.execute_reply":"2024-10-25T20:18:44.173734Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#data2 = pd.concat([pd.DataFrame(xx_train), pd.DataFrame(yy_train)], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.176856Z","iopub.execute_input":"2024-10-25T20:18:44.177389Z","iopub.status.idle":"2024-10-25T20:18:44.183153Z","shell.execute_reply.started":"2024-10-25T20:18:44.177335Z","shell.execute_reply":"2024-10-25T20:18:44.181964Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# MICE:","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\n\nimp = sm.MICEData(data)\ndata = imp.data\ndata","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:44.184363Z","iopub.execute_input":"2024-10-25T20:18:44.184787Z","iopub.status.idle":"2024-10-25T20:18:46.129349Z","shell.execute_reply.started":"2024-10-25T20:18:44.184748Z","shell.execute_reply":"2024-10-25T20:18:46.128043Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      Sex  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n0       1   39        4.0              0         0.0     0.0                0   \n1       0   46        2.0              0         0.0     0.0                0   \n2       1   48        1.0              1        20.0     0.0                0   \n3       0   61        3.0              1        30.0     0.0                0   \n4       0   46        3.0              1        23.0     0.0                0   \n...   ...  ...        ...            ...         ...     ...              ...   \n4235    0   48        2.0              1        20.0     0.0                0   \n4236    0   44        1.0              1        15.0     0.0                0   \n4237    0   52        2.0              0         0.0     0.0                0   \n4238    1   40        3.0              0         0.0     0.0                0   \n4239    0   39        3.0              1        30.0     0.0                0   \n\n      prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  \\\n0                0         0    195.0  106.0   70.0  26.97       80.0   \n1                0         0    250.0  121.0   81.0  28.73       95.0   \n2                0         0    245.0  127.5   80.0  25.34       75.0   \n3                1         0    225.0  150.0   95.0  28.58       65.0   \n4                0         0    285.0  130.0   84.0  23.10       85.0   \n...            ...       ...      ...    ...    ...    ...        ...   \n4235             0         0    248.0  131.0   72.0  22.00       84.0   \n4236             0         0    210.0  126.5   87.0  19.16       86.0   \n4237             0         0    269.0  133.5   83.0  21.47       80.0   \n4238             1         0    185.0  141.0   98.0  25.60       67.0   \n4239             0         0    196.0  133.0   86.0  20.91       85.0   \n\n      glucose  TenYearCHD  \n0        77.0           0  \n1        76.0           0  \n2        70.0           0  \n3       103.0           1  \n4        85.0           0  \n...       ...         ...  \n4235     86.0           0  \n4236     82.0           0  \n4237    107.0           0  \n4238     72.0           0  \n4239     80.0           0  \n\n[4240 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n      <th>TenYearCHD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>39</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>48</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>46</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4235</th>\n      <td>0</td>\n      <td>48</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248.0</td>\n      <td>131.0</td>\n      <td>72.0</td>\n      <td>22.00</td>\n      <td>84.0</td>\n      <td>86.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4236</th>\n      <td>0</td>\n      <td>44</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>210.0</td>\n      <td>126.5</td>\n      <td>87.0</td>\n      <td>19.16</td>\n      <td>86.0</td>\n      <td>82.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4237</th>\n      <td>0</td>\n      <td>52</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>269.0</td>\n      <td>133.5</td>\n      <td>83.0</td>\n      <td>21.47</td>\n      <td>80.0</td>\n      <td>107.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4238</th>\n      <td>1</td>\n      <td>40</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>185.0</td>\n      <td>141.0</td>\n      <td>98.0</td>\n      <td>25.60</td>\n      <td>67.0</td>\n      <td>72.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4239</th>\n      <td>0</td>\n      <td>39</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>196.0</td>\n      <td>133.0</td>\n      <td>86.0</td>\n      <td>20.91</td>\n      <td>85.0</td>\n      <td>80.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4240 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Imputure","metadata":{}},{"cell_type":"code","source":"# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n\n# df= pd.DataFrame(data)\n# imputer= IterativeImputer(max_iter=3, random_state=42)\n\n# impute_data= imputer.fit_transform(df)\n# data= pd.DataFrame(impute_data, columns= df.columns)\n\n# data","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.130858Z","iopub.execute_input":"2024-10-25T20:18:46.131715Z","iopub.status.idle":"2024-10-25T20:18:46.137311Z","shell.execute_reply.started":"2024-10-25T20:18:46.131671Z","shell.execute_reply":"2024-10-25T20:18:46.135767Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.139246Z","iopub.execute_input":"2024-10-25T20:18:46.139704Z","iopub.status.idle":"2024-10-25T20:18:46.167405Z","shell.execute_reply.started":"2024-10-25T20:18:46.139653Z","shell.execute_reply":"2024-10-25T20:18:46.166183Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4240 entries, 0 to 4239\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Sex              4240 non-null   int64  \n 1   age              4240 non-null   int64  \n 2   education        4240 non-null   float64\n 3   currentSmoker    4240 non-null   int64  \n 4   cigsPerDay       4240 non-null   float64\n 5   BPMeds           4240 non-null   float64\n 6   prevalentStroke  4240 non-null   int64  \n 7   prevalentHyp     4240 non-null   int64  \n 8   diabetes         4240 non-null   int64  \n 9   totChol          4240 non-null   float64\n 10  sysBP            4240 non-null   float64\n 11  diaBP            4240 non-null   float64\n 12  BMI              4240 non-null   float64\n 13  heartRate        4240 non-null   float64\n 14  glucose          4240 non-null   float64\n 15  TenYearCHD       4240 non-null   int64  \ndtypes: float64(9), int64(7)\nmemory usage: 530.1 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"data.corr()['TenYearCHD'].sort_values(ascending= False)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.168877Z","iopub.execute_input":"2024-10-25T20:18:46.169342Z","iopub.status.idle":"2024-10-25T20:18:46.187572Z","shell.execute_reply.started":"2024-10-25T20:18:46.169283Z","shell.execute_reply":"2024-10-25T20:18:46.186421Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TenYearCHD         1.000000\nage                0.225408\nsysBP              0.216374\nprevalentHyp       0.177458\ndiaBP              0.145112\nglucose            0.120442\ndiabetes           0.097344\nSex                0.088374\nBPMeds             0.086448\ntotChol            0.081813\nBMI                0.074787\nprevalentStroke    0.061823\ncigsPerDay         0.057647\nheartRate          0.022897\ncurrentSmoker      0.019448\neducation         -0.053570\nName: TenYearCHD, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# x_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# classifiers = {\n#     \"RandomForest\": RandomForestClassifier(random_state=42),\n#     \"AdaBoost\": AdaBoostClassifier(random_state=42),\n#     \"ExtraTrees\": ExtraTreesClassifier(n_estimators=1500, random_state=42),\n#     \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n#     \"Bagging\": BaggingClassifier(random_state=42),\n#     \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n#     \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n#     \"LogisticRegression\": LogisticRegression(random_state=42),\n#     \"GaussianNB\": GaussianNB(),\n#     \"BernoulliNB\": BernoulliNB(),\n#     \"MultinomialNB\": MultinomialNB(),\n#     \"KNeighbors\": KNeighborsClassifier(),\n#     #\"RadiusNeighbors\": RadiusNeighborsClassifier(),\n#     \"SVC\": SVC(probability=True, random_state=42),\n#     \"XGBoost\": XGBClassifier(random_state=42),\n#     \"LightGBM\": LGBMClassifier(random_state=42),\n#     \"MLPClassifier\": MLPClassifier(random_state=42)\n# }\n# results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'Specificity', 'ROC-AUC', 'Confusion Matrix'])\n\n# def evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test):\n#     global results_df\n#     for name, clf in classifiers.items():\n#         try:\n#             clf.fit(x_train, y_train) \n#             y_pred = clf.predict(x_test)  \n\n#             cm = confusion_matrix(y_test, y_pred)\n#             if len(cm.ravel()) == 4:  \n#                 tn, fp, fn, tp = cm.ravel()\n#                 specificity = tn / (tn + fp) \n#             else:\n#                 specificity = None\n\n#             acc = accuracy_score(y_test, y_pred)\n#             f1 = f1_score(y_test, y_pred, average='weighted')\n#             report = classification_report(y_test, y_pred, output_dict=True)\n#             precision = report['weighted avg']['precision']\n#             recall = report['weighted avg']['recall']\n\n#             if hasattr(clf, \"predict_proba\"): \n#                 y_proba = clf.predict_proba(x_test)[:, 1]\n#                 roc_auc = roc_auc_score(y_test, y_proba)\n#             else:\n#                 roc_auc = None\n                \n#             new_row = pd.DataFrame({\n#                 'Model': [name],\n#                 'Accuracy': [acc],\n#                 'F1 Score': [f1],\n#                 'Precision': [precision],\n#                 'Recall': [recall],\n#                 'Specificity': [specificity],\n#                 'ROC-AUC': [roc_auc],\n#                 'Confusion Matrix': [cm]\n#             })\n#             results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n#         except (ValueError, NotFittedError) as e:\n#             print(f\"Error with classifier {name}: {e}\")\n            \n# evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test)\n# print(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.189314Z","iopub.execute_input":"2024-10-25T20:18:46.189675Z","iopub.status.idle":"2024-10-25T20:18:46.197434Z","shell.execute_reply.started":"2024-10-25T20:18:46.189637Z","shell.execute_reply":"2024-10-25T20:18:46.196130Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# def compute_knn(X, y, minority_class, k):\n#     minority_instances = X[y == minority_class]\n#     nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n#     distances, indices = nbrs.kneighbors(minority_instances)\n#     return indices\n\n# def identify_borderline_instances(X, y, indices, minority_class, k):\n#     borderline_instances = []\n#     for i, idx in enumerate(indices):\n#         ksame = sum(y[idx] == minority_class)\n#         kmajority = k - ksame\n#         if ksame < k and kmajority > ksame:\n#             borderline_instances.append(i)\n#     return borderline_instances\n\n\n# def generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples):\n#     synthetic_samples = []\n#     num_borderline = len(borderline_indices)\n#     # প্রতি বর্ডারলাইন ইনস্ট্যান্স থেকে কিছু সিনথেটিক ডেটা তৈরি করবো\n#     samples_per_instance = num_samples // num_borderline\n#     for i in borderline_indices:\n#         for _ in range(samples_per_instance):\n#             rand_idx = np.random.choice(indices[i])\n#             xj = X[rand_idx]\n#             xi = X[i]\n#             # Interpolation\n#             rand = np.random.rand()\n#             xnew = xi + rand * (xi - xj)\n#             synthetic_samples.append(xnew)\n#     # কিছু বাকি ডেটা যদি থেকে যায় (যদি ভাগফল পূর্ণ সংখ্যা না হয়)\n#     remaining_samples = num_samples - len(synthetic_samples)\n#     for _ in range(remaining_samples):\n#         i = np.random.choice(borderline_indices)\n#         rand_idx = np.random.choice(indices[i])\n#         xj = X[rand_idx]\n#         xi = X[i]\n#         rand = np.random.rand()\n#         xnew = xi + rand * (xi - xj)\n#         synthetic_samples.append(xnew)\n#     return np.array(synthetic_samples)\n\n\n# def smote(X, y, minority_class, k, num_samples):\n#     indices = compute_knn(X, y, minority_class, k)\n#     borderline_indices = identify_borderline_instances(X, y, indices, minority_class, k)\n#     synthetic_samples = generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples)\n    \n#     X_new = np.vstack((X, synthetic_samples))\n#     y_new = np.hstack((y, np.array([minority_class] * len(synthetic_samples))))\n    \n#     return X_new, y_new\n\n# if __name__ == \"__main__\":\n#     target = data['TenYearCHD']\n#     columns_to_drop = ['TenYearCHD']\n#     features = data.drop(columns_to_drop, axis=1)\n\n#     X = features.values\n#     y = target.values\n\n#     count_zeros_original = np.sum(y == 0)\n#     count_ones_original = np.sum(y == 1)\n\n#     print(f\"Original Count of 0s (No Heart Attack): {count_zeros_original}\")\n#     print(f\"Original Count of 1s (Heart Attack): {count_ones_original}\")\n\n#     minority_class = 1\n#     k = 5\n#     num_samples = 333\n\n#     X_new, y_new = smote(X, y, minority_class, k, num_samples)\n\n#     x_train, x_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n\n#     model = RandomForestClassifier()\n#     model.fit(x_train, y_train)\n\n#     y_pred = model.predict(x_test)\n#     print(classification_report(y_test, y_pred))\n\n#     count_zeros_new = np.sum(y_new == 0)\n#     count_ones_new = np.sum(y_new == 1)\n\n#     print(f\"Count of 0s (No Heart Attack): {count_zeros_new}\")\n#     print(f\"Count of 1s (Heart Attack): {count_ones_new}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.198901Z","iopub.execute_input":"2024-10-25T20:18:46.199296Z","iopub.status.idle":"2024-10-25T20:18:46.212496Z","shell.execute_reply.started":"2024-10-25T20:18:46.199257Z","shell.execute_reply":"2024-10-25T20:18:46.211417Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# def compute_knn(X, y, minority_class, k):\n#     minority_instances = X[y == minority_class]\n#     nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n#     distances, indices = nbrs.kneighbors(minority_instances)\n#     return indices\n\n# def identify_borderline_instances(X, y, indices, minority_class, k):\n#     borderline_instances = []\n#     for i, idx in enumerate(indices):\n#         ksame = sum(y[idx] == minority_class)\n#         kmajority = k - ksame\n#         if ksame < k and kmajority > ksame:\n#             borderline_instances.append(i)\n#     return borderline_instances\n\n# def generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples):\n#     synthetic_samples = []\n#     num_borderline = len(borderline_indices)\n\n#     samples_per_instance = num_samples // num_borderline\n#     for i in borderline_indices:\n#         for _ in range(samples_per_instance):\n#             rand_idx = np.random.choice(indices[i])\n#             xj = X[rand_idx]\n#             xi = X[i]\n#             rand = np.random.rand()\n#             xnew = xi + rand * (xj - xi)\n#             synthetic_samples.append(xnew)\n\n#     remaining_samples = num_samples - len(synthetic_samples)\n#     for _ in range(remaining_samples):\n#         i = np.random.choice(borderline_indices)\n#         rand_idx = np.random.choice(indices[i])\n#         xj = X[rand_idx]\n#         xi = X[i]\n#         rand = np.random.rand()\n#         xnew = xi + rand * (xj - xi)\n#         synthetic_samples.append(xnew)\n#     return np.array(synthetic_samples)\n\n# def smote(X, y, minority_class, k, num_samples):\n#     indices = compute_knn(X, y, minority_class, k)\n \n#     borderline_indices = identify_borderline_instances(X, y, indices, minority_class, k)\n#     synthetic_samples = generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples)\n    \n#     X_new = np.vstack((X, synthetic_samples))\n#     y_new = np.hstack((y, np.array([minority_class] * len(synthetic_samples))))\n    \n#     return X_new, y_new\n\n# if __name__ == \"__main__\":\n#     target = data['TenYearCHD']\n#     columns_to_drop = ['TenYearCHD']\n#     features = data.drop(columns_to_drop, axis=1)\n\n#     X = features.values\n#     y = target.values\n\n#     count_zeros_original = np.sum(y == 0)\n#     count_ones_original = np.sum(y == 1)\n\n#     print(f\"Original Count of 0s (No Heart Attack): {count_zeros_original}\")\n#     print(f\"Original Count of 1s (Heart Attack): {count_ones_original}\")\n\n#     minority_class = 1\n#     k = 5\n#     num_samples = 333\n\n#     X_new, y_new = smote(X, y, minority_class, k, num_samples)\n\n#     x_train, x_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n\n#     model = ExtraTreesClassifier()\n#     model.fit(x_train, y_train)\n\n#     y_pred = model.predict(x_test)\n#     print(classification_report(y_test, y_pred))\n    \n#     count_zeros_new = np.sum(y_new == 0)\n#     count_ones_new = np.sum(y_new == 1)\n\n#     print(f\"Count of 0s (No Heart Attack): {count_zeros_new}\")\n#     print(f\"Count of 1s (Heart Attack): {count_ones_new}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.216436Z","iopub.execute_input":"2024-10-25T20:18:46.216807Z","iopub.status.idle":"2024-10-25T20:18:46.229723Z","shell.execute_reply.started":"2024-10-25T20:18:46.216768Z","shell.execute_reply":"2024-10-25T20:18:46.228636Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# def compute_knn(X, y, minority_class, k):\n#     minority_instances = X[y == minority_class]\n#     nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n#     distances, indices = nbrs.kneighbors(minority_instances)\n#     return indices\n\n# def identify_borderline_instances(X, y, indices, minority_class, k):\n#     borderline_instances = []\n#     for i, idx in enumerate(indices):\n#         ksame = sum(y[idx] == minority_class)\n#         kmajority = k - ksame\n#         if ksame < k and kmajority > ksame:\n#             borderline_instances.append(i)\n#     return borderline_instances\n\n# def generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples):\n#     synthetic_samples = []\n#     num_borderline = len(borderline_indices)\n\n#     samples_per_instance = num_samples // num_borderline\n#     for i in borderline_indices:\n#         for _ in range(samples_per_instance):\n#             rand_idx = np.random.choice(indices[i])\n#             xj = X[rand_idx]\n#             xi = X[i]\n#             rand = np.random.rand()\n#             xnew = xi + rand * (xj - xi)\n#             synthetic_samples.append(xnew)\n\n#     remaining_samples = num_samples - len(synthetic_samples)\n#     for _ in range(remaining_samples):\n#         i = np.random.choice(borderline_indices)\n#         rand_idx = np.random.choice(indices[i])\n#         xj = X[rand_idx]\n#         xi = X[i]\n#         rand = np.random.rand()\n#         xnew = xi + rand * (xj - xi)\n#         synthetic_samples.append(xnew)\n#     return np.array(synthetic_samples)\n\n# def smote(X, y, minority_class, k, num_samples):\n#     indices = compute_knn(X, y, minority_class, k)\n#     borderline_indices = identify_borderline_instances(X, y, indices, minority_class, k)\n#     synthetic_samples = generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples)\n    \n#     X_new = np.vstack((X, synthetic_samples))\n#     y_new = np.hstack((y, np.array([minority_class] * len(synthetic_samples))))\n    \n#     return X_new, y_new\n\n# if __name__ == \"__main__\":\n#     target = data['TenYearCHD']\n#     columns_to_drop = ['TenYearCHD']\n#     features = data.drop(columns_to_drop, axis=1)\n#     X = features.values\n#     y = target.values\n\n#     count_zeros_original = np.sum(y == 0)\n#     count_ones_original = np.sum(y == 1)\n#     print(f\"0s (No Heart Attack): {count_zeros_original}\")\n#     print(f\"1s (Heart Attack): {count_ones_original}\")\n    \n#     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#     print(f\"Training size: {x_train.shape}, Test size: {x_test.shape}\")\n    \n#     minority_class =1\n#     k =5\n#     num_samples = 2000\n#     x_train_new, y_train_new = smote(x_train, y_train, minority_class, k, num_samples)\n#     print(f\"New training size: {x_train_new.shape}, Original training size: {x_train.shape}\")\n\n#     model = ExtraTreesClassifier(class_weight='balanced')\n#     model.fit(x_train_new, y_train_new)\n#     y_pred = model.predict(x_test)\n#     print(classification_report(y_test, y_pred))\n\n#     count_zeros_new = np.sum(y_train_new == 0)\n#     count_ones_new = np.sum(y_train_new == 1)\n\n#     print(f\"0s (No Heart Attack) in new training: {count_zeros_new}\")\n#     print(f\"1s (Heart Attack) in new training: {count_ones_new}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.232167Z","iopub.execute_input":"2024-10-25T20:18:46.232528Z","iopub.status.idle":"2024-10-25T20:18:46.247247Z","shell.execute_reply.started":"2024-10-25T20:18:46.232490Z","shell.execute_reply":"2024-10-25T20:18:46.245750Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# ADASYN","metadata":{}},{"cell_type":"code","source":"# from imblearn.over_sampling import ADASYN\n\n# def compute_knn(X, y, minority_class, k):\n#     minority_instances = X[y == minority_class]\n#     nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n#     distances, indices = nbrs.kneighbors(minority_instances)\n#     return indices\n\n# def identify_borderline_instances(X, y, indices, minority_class, k):\n#     borderline_instances = []\n#     for i, idx in enumerate(indices):\n#         ksame = sum(y[idx] == minority_class)\n#         kmajority = k - ksame\n#         if ksame < k and kmajority > ksame:\n#             borderline_instances.append(i)\n#     return borderline_instances\n\n# def generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples):\n#     synthetic_samples = []\n#     num_borderline = len(borderline_indices)\n#     samples_per_instance = num_samples // num_borderline\n#     for i in borderline_indices:\n#         for _ in range(samples_per_instance):\n#             rand_idx = np.random.choice(indices[i])\n#             xj = X[rand_idx]\n#             xi = X[i]\n#             # Interpolation\n#             rand = np.random.rand()\n#             xnew = xi + rand * (xi - xj)\n#             synthetic_samples.append(xnew)\n#     remaining_samples = num_samples - len(synthetic_samples)\n#     for _ in range(remaining_samples):\n#         i = np.random.choice(borderline_indices)\n#         rand_idx = np.random.choice(indices[i])\n#         xj = X[rand_idx]\n#         xi = X[i]\n#         rand = np.random.rand()\n#         xnew = xi + rand * (xi - xj)\n#         synthetic_samples.append(xnew)\n#     return np.array(synthetic_samples)\n\n\n# def smote(X, y, minority_class, k, num_samples):\n#     indices = compute_knn(X, y, minority_class, k)\n#     borderline_indices = identify_borderline_instances(X, y, indices, minority_class, k)\n#     synthetic_samples = generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples)\n    \n#     X_new = np.vstack((X, synthetic_samples))\n#     y_new = np.hstack((y, np.array([minority_class] * len(synthetic_samples))))\n    \n#     return X_new, y_new\n\n# if __name__ == \"__main__\":\n    \n#     target = data['TenYearCHD']\n#     columns_to_drop = ['TenYearCHD']\n#     features = data.drop(columns_to_drop, axis=1)\n\n#     X = features.values\n#     y = target.values\n\n#     count_zeros_original = np.sum(y == 0)\n#     count_ones_original = np.sum(y == 1)\n\n#     print(f\"Original Count of 0s (No Heart Attack): {count_zeros_original}\")\n#     print(f\"Original Count of 1s (Heart Attack): {count_ones_original}\")\n\n#     ada = ADASYN(sampling_strategy='minority', random_state=42)\n#     X_new, y_new = ada.fit_resample(X, y)\n\n#     x_train, x_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n\n#     model = RandomForestClassifier(random_state=42)\n#     model.fit(x_train, y_train)\n#     y_pred = model.predict(x_test)\n#     print(classification_report(y_test, y_pred))\n\n#     count_zeros_new = np.sum(y_new == 0)\n#     count_ones_new = np.sum(y_new == 1)\n\n#     print(f\"Count of 0s (No Heart Attack): {count_zeros_new}\")\n#     print(f\"Count of 1s (Heart Attack): {count_ones_new}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:46.248996Z","iopub.execute_input":"2024-10-25T20:18:46.249441Z","iopub.status.idle":"2024-10-25T20:18:46.264512Z","shell.execute_reply.started":"2024-10-25T20:18:46.249382Z","shell.execute_reply":"2024-10-25T20:18:46.263389Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Combining Oversampling and Undersampling","metadata":{}},{"cell_type":"code","source":"data= pd.read_csv('/kaggle/input/framingham/framingham.csv')\ndata['Sex'] = data['Sex'].map({'male': 1, 'female': 0})\ndata['currentSmoker']= data['currentSmoker'].map({'Yes':1, 'No':0})\ndata['diabetes']= data['diabetes'].map({'Yes':1, 'No':0})\ndata['cigsPerDay'] = data['cigsPerDay'].fillna(data['cigsPerDay'].mean())\ndata['BPMeds'] = data['BPMeds'].fillna(data['BPMeds'].median())\ndata['totChol'] = data['totChol'].fillna(data['totChol'].mean())\ndata['BMI'] = data['BMI'].fillna(data['BMI'].median())\ndata['heartRate'] = data['heartRate'].fillna(data['heartRate'].median())\ndata['glucose'] = data['glucose'].fillna(data['glucose'].median())\ndata['education'] = data['education'].fillna(data['education'].mode()[0])\n# data\ndef remove_outliers_iqr(df, column):\n    Q1= df[column].quantile(0.25)  \n    Q3= df[column].quantile(0.75) \n    IQR= Q3- Q1                   \n    lower_bound=Q1- 1.5* IQR    \n    upper_bound= Q3+ 1.5* IQR     \n\n    return df[(df[column]>= lower_bound) & (df[column]<= upper_bound)]\n\ncolumns_with_outliers= ['cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'glucose']\nfor col in columns_with_outliers:\n    data = remove_outliers_iqr(data, col)\n\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:56:34.648761Z","iopub.execute_input":"2024-10-25T20:56:34.649314Z","iopub.status.idle":"2024-10-25T20:56:34.720256Z","shell.execute_reply.started":"2024-10-25T20:56:34.649260Z","shell.execute_reply":"2024-10-25T20:56:34.718974Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"(3655, 16)\n","output_type":"stream"}]},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nif __name__ == \"__main__\":   \n    target = data['TenYearCHD']\n    columns_to_drop = ['TenYearCHD']\n    features = data.drop(columns_to_drop, axis=1)\n\n    X = features.values\n    y = target.values\n    \n    count_zeros_original = np.sum(y == 0)\n    count_ones_original = np.sum(y == 1)\n\n    print(f\"Original Count of 0s (No Heart Attack): {count_zeros_original}\")\n    print(f\"Original Count of 1s (Heart Attack): {count_ones_original}\")\n\n    majority_count = count_zeros_original  \n    minority_count = count_ones_original   \n\n    desired_minority_count = majority_count\n    num_samples_to_oversample = desired_minority_count - minority_count-100\n\n    print(f\"Desired Count of 1s after oversampling: {desired_minority_count}\")\n    print(f\"Number of 1s to oversample: {num_samples_to_oversample}\")\n\n    over = ADASYN(sampling_strategy={1: desired_minority_count}, random_state=63)  \n    under = RandomUnderSampler(sampling_strategy={0: majority_count}, random_state=63)  \n\n    pipeline = Pipeline(steps=[('over', over), ('under', under)])\n\n    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n    x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n    model = RandomForestClassifier(random_state=42)\n    model.fit(x_train, y_train)\n\n    y_pred_proba = model.predict_proba(x_test)[:, 1]\n    threshold = 0.5\n    y_pred_adjusted = (y_pred_proba >= threshold).astype(int)\n\n    print(classification_report(y_test, y_pred_adjusted))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:10:23.792432Z","iopub.execute_input":"2024-10-25T21:10:23.792883Z","iopub.status.idle":"2024-10-25T21:10:24.891322Z","shell.execute_reply.started":"2024-10-25T21:10:23.792839Z","shell.execute_reply":"2024-10-25T21:10:24.890094Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Original Count of 0s (No Heart Attack): 3156\nOriginal Count of 1s (Heart Attack): 499\nDesired Count of 1s after oversampling: 3156\nNumber of 1s to oversample: 2557\n              precision    recall  f1-score   support\n\n           0       0.90      0.94      0.92       644\n           1       0.94      0.90      0.92       636\n\n    accuracy                           0.92      1280\n   macro avg       0.92      0.92      0.92      1280\nweighted avg       0.92      0.92      0.92      1280\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ExtraTreesClassifier(random_state=42, class_weight={0: 1, 1: 7})\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:10:28.257145Z","iopub.execute_input":"2024-10-25T21:10:28.257555Z","iopub.status.idle":"2024-10-25T21:10:29.020836Z","shell.execute_reply.started":"2024-10-25T21:10:28.257517Z","shell.execute_reply":"2024-10-25T21:10:29.019663Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.94      0.97      0.95       644\n           1       0.97      0.94      0.95       636\n\n    accuracy                           0.95      1280\n   macro avg       0.95      0.95      0.95      1280\nweighted avg       0.95      0.95      0.95      1280\n\n","output_type":"stream"}]},{"cell_type":"code","source":"train_preds = model.predict(x_train)\ntrain_accuracy = accuracy_score(y_train, train_preds)\nprint(\"Training Accuracy: {:.2f}%\".format(train_accuracy * 100))\ntest_preds = model.predict(x_test)\ntest_accuracy = accuracy_score(y_test, test_preds)\nprint(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:10:32.047014Z","iopub.execute_input":"2024-10-25T21:10:32.047476Z","iopub.status.idle":"2024-10-25T21:10:32.234947Z","shell.execute_reply.started":"2024-10-25T21:10:32.047437Z","shell.execute_reply":"2024-10-25T21:10:32.233527Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"Training Accuracy: 100.00%\nTest Accuracy: 95.31%\n","output_type":"stream"}]},{"cell_type":"code","source":"classifiers = {\n    \"RandomForest\": RandomForestClassifier(random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=1500, random_state=42, class_weight={0: 1, 1: 5}),\n    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n    \"Bagging\": BaggingClassifier(random_state=42),\n    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n    \"LogisticRegression\": LogisticRegression(random_state=42),\n    \"GaussianNB\": GaussianNB(),\n    \"BernoulliNB\": BernoulliNB(),\n    \"MultinomialNB\": MultinomialNB(),\n    \"KNeighbors\": KNeighborsClassifier(),\n    #\"RadiusNeighbors\": RadiusNeighborsClassifier(),\n    \"SVC\": SVC(probability=True, random_state=42),\n    \"XGBoost\": XGBClassifier(random_state=42),\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    \"MLPClassifier\": MLPClassifier(random_state=42)\n}\nresults_df = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'Specificity', 'ROC-AUC', 'Confusion Matrix'])\n\ndef evaluate_classifiers(classifiers, x_train, x_test, y_train_resampled, y_test):\n    global results_df\n    for name, clf in classifiers.items():\n        try:\n            clf.fit(x_train, y_train) \n            y_pred = clf.predict(x_test)  \n\n            cm = confusion_matrix(y_test, y_pred)\n            if len(cm.ravel()) == 4:  \n                tn, fp, fn, tp = cm.ravel()\n                specificity = tn / (tn + fp) \n            else:\n                specificity = None\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred, average='weighted')\n            report = classification_report(y_test, y_pred, output_dict=True)\n            precision = report['weighted avg']['precision']\n            recall = report['weighted avg']['recall']\n\n            if hasattr(clf, \"predict_proba\"): \n                y_proba = clf.predict_proba(x_test)[:, 1]\n                roc_auc = roc_auc_score(y_test, y_proba)\n            else:\n                roc_auc = None\n                \n            new_row = pd.DataFrame({\n                'Model': [name],\n                'Accuracy': [acc],\n                'F1 Score': [f1],\n                'Precision': [precision],\n                'Recall': [recall],\n                'Specificity': [specificity],\n                'ROC-AUC': [roc_auc],\n                'Confusion Matrix': [cm]\n            })\n            results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n        except (ValueError, NotFittedError) as e:\n            print(f\"Error with classifier {name}: {e}\")\n            \nevaluate_classifiers(classifiers, x_train, x_test, y_train, y_test)\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:10:36.469332Z","iopub.execute_input":"2024-10-25T21:10:36.469735Z","iopub.status.idle":"2024-10-25T21:11:02.139419Z","shell.execute_reply.started":"2024-10-25T21:10:36.469699Z","shell.execute_reply":"2024-10-25T21:11:02.137906Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2139533368.py:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results_df = pd.concat([results_df, new_row], ignore_index=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2604, number of negative: 2512\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001834 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3084\n[LightGBM] [Info] Number of data points in the train set: 5116, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508991 -> initscore=0.035969\n[LightGBM] [Info] Start training from score 0.035969\n                   Model  Accuracy  F1 Score  Precision    Recall  \\\n0           RandomForest  0.917969  0.917901   0.919057  0.917969   \n1               AdaBoost  0.861719  0.861426   0.864338  0.861719   \n2             ExtraTrees  0.953906  0.953900   0.954035  0.953906   \n3       GradientBoosting  0.885938  0.885120   0.896308  0.885938   \n4                Bagging  0.907813  0.907672   0.909891  0.907813   \n5   HistGradientBoosting  0.908594  0.908312   0.913067  0.908594   \n6           DecisionTree  0.839844  0.839833   0.840054  0.839844   \n7     LogisticRegression  0.640625  0.640614   0.640717  0.640625   \n8             GaussianNB  0.594531  0.577203   0.611270  0.594531   \n9            BernoulliNB  0.664844  0.664822   0.664841  0.664844   \n10         MultinomialNB  0.616406  0.615751   0.616908  0.616406   \n11            KNeighbors  0.814063  0.808197   0.859958  0.814063   \n12                   SVC  0.630469  0.629789   0.631800  0.630469   \n13               XGBoost  0.907031  0.906820   0.910237  0.907031   \n14              LightGBM  0.907813  0.907568   0.911624  0.907813   \n15         MLPClassifier  0.642969  0.639962   0.647172  0.642969   \n\n    Specificity   ROC-AUC          Confusion Matrix  \n0      0.944099  0.975718    [[608, 36], [69, 567]]  \n1      0.905280  0.927719   [[583, 61], [116, 520]]  \n2      0.962733  0.989100    [[620, 24], [35, 601]]  \n3      0.967391  0.943789   [[623, 21], [125, 511]]  \n4      0.944099  0.955799    [[608, 36], [82, 554]]  \n5      0.961180  0.957672    [[619, 25], [92, 544]]  \n6      0.829193  0.839911   [[534, 110], [95, 541]]  \n7      0.633540  0.688894  [[408, 236], [224, 412]]  \n8      0.795031  0.692643  [[512, 132], [387, 249]]  \n9      0.672360  0.724121  [[433, 211], [218, 418]]  \n10     0.656832  0.639366  [[423, 221], [270, 366]]  \n11     0.638199  0.915353    [[411, 233], [5, 631]]  \n12     0.586957  0.687397  [[378, 266], [207, 429]]  \n13     0.951863  0.957222    [[613, 31], [88, 548]]  \n14     0.956522  0.956441    [[616, 28], [90, 546]]  \n15     0.732919  0.707752  [[472, 172], [285, 351]]  \n","output_type":"stream"}]},{"cell_type":"code","source":"# results_df.to_csv('model_evaluation_results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:47.253751Z","iopub.status.idle":"2024-10-25T20:18:47.254287Z","shell.execute_reply.started":"2024-10-25T20:18:47.254031Z","shell.execute_reply":"2024-10-25T20:18:47.254058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\ndef compute_knn(X, y, minority_class, k):\n    minority_instances = X[y == minority_class]\n    nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n    distances, indices = nbrs.kneighbors(minority_instances)\n    return indices\n\ndef identify_borderline_instances(X, y, indices, minority_class, k):\n    borderline_instances = []\n    for i, idx in enumerate(indices):\n        ksame = sum(y[idx] == minority_class)\n        kmajority = k - ksame\n        if ksame < k and kmajority > ksame:\n            borderline_instances.append(i)\n    return borderline_instances\n\ndef generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples):\n    synthetic_samples = []\n    num_borderline = len(borderline_indices)\n    samples_per_instance = num_samples // num_borderline\n    for i in borderline_indices:\n        for _ in range(samples_per_instance):\n            rand_idx = np.random.choice(indices[i])\n            xj = X[rand_idx]\n            xi = X[i]\n            rand = np.random.rand()\n            xnew = xi + rand * (xj - xi)\n            synthetic_samples.append(xnew)\n    remaining_samples = num_samples - len(synthetic_samples)\n    for _ in range(remaining_samples):\n        i = np.random.choice(borderline_indices)\n        rand_idx = np.random.choice(indices[i])\n        xj = X[rand_idx]\n        xi = X[i]\n        rand = np.random.rand()\n        xnew = xi + rand * (xj - xi)\n        synthetic_samples.append(xnew)\n    return np.array(synthetic_samples)\n\ndef smote(X, y, minority_class, k, num_samples):\n    indices = compute_knn(X, y, minority_class, k)\n    borderline_indices = identify_borderline_instances(X, y, indices, minority_class, k)\n    synthetic_samples = generate_synthetic_samples(X, y, borderline_indices, indices, minority_class, num_samples)\n    X_new = np.vstack((X, synthetic_samples))\n    y_new = np.hstack((y, np.array([minority_class] * len(synthetic_samples))))\n    return X_new, y_new\n\ndef build_autoencoder(input_dim, encoding_dim):\n    input_layer = Input(shape=(input_dim,))\n    encoder = Dense(encoding_dim, activation='relu')(input_layer)\n    decoder = Dense(input_dim, activation='linear')(encoder)\n    autoencoder = Model(inputs=input_layer, outputs=decoder)\n    autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n    return autoencoder\n\nif __name__ == \"__main__\":\n    target = data['TenYearCHD']\n    columns_to_drop = ['TenYearCHD']\n    features = data.drop(columns_to_drop, axis=1)\n    X = features.values\n    y = target.values\n\n    count_zeros_original = np.sum(y == 0)\n    count_ones_original = np.sum(y == 1)\n    print(f\"Original Count of 0s (No Heart Attack): {count_zeros_original}\")\n    print(f\"Original Count of 1s (Heart Attack): {count_ones_original}\")\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    input_dim = X_scaled.shape[1]\n    encoding_dim = input_dim // 2  \n    \n    autoencoder = build_autoencoder(input_dim, encoding_dim)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    autoencoder.fit(X_scaled, X_scaled, epochs=200, batch_size=16, validation_split=0.2, verbose=2, callbacks=[early_stopping])\n    \n    X_imputed = autoencoder.predict(X_scaled)\n    X_imputed = scaler.inverse_transform(X_imputed)\n\n    X_final = np.where(np.isnan(X), X_imputed, X)\n\n    over = FunctionSampler(func=ADASYN(sampling_strategy='minority', random_state=42).fit_resample)\n    under = FunctionSampler(func=RandomUnderSampler(sampling_strategy='majority', random_state=42).fit_resample)\n\n    pipeline = Pipeline(steps=[('over', over), ('under', under)])\n    \n    X_resampled, y_resampled = pipeline.fit_resample(X_final, y)\n    k = 5 \n    num_samples = 333 \n    minority_class = 1  \n\n    X_final_resampled, y_final_resampled = smote(X_resampled, y_resampled, minority_class, k, num_samples)\n    x_train, x_test, y_train, y_test = train_test_split(X_final_resampled, y_final_resampled, test_size=0.2, random_state=42)\n\n    model = RandomForestClassifier(random_state=42)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    print(classification_report(y_test, y_pred))\n\n    count_zeros_new = np.sum(y_final_resampled == 0)\n    count_ones_new = np.sum(y_final_resampled == 1)\n    print(f\"Final Count of 0s (No Heart Attack) in new training set: {count_zeros_new}\")\n    print(f\"Final Count of 1s (Heart Attack) in new training set: {count_ones_new}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:06:10.751173Z","iopub.execute_input":"2024-10-25T21:06:10.751599Z","iopub.status.idle":"2024-10-25T21:06:31.028106Z","shell.execute_reply.started":"2024-10-25T21:06:10.751560Z","shell.execute_reply":"2024-10-25T21:06:31.026920Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Original Count of 0s (No Heart Attack): 3156\nOriginal Count of 1s (Heart Attack): 499\nEpoch 1/200\n183/183 - 1s - 7ms/step - loss: 1.1249 - val_loss: 0.9809\nEpoch 2/200\n183/183 - 0s - 2ms/step - loss: 0.8345 - val_loss: 0.7939\nEpoch 3/200\n183/183 - 0s - 2ms/step - loss: 0.6911 - val_loss: 0.6706\nEpoch 4/200\n183/183 - 0s - 2ms/step - loss: 0.5912 - val_loss: 0.5799\nEpoch 5/200\n183/183 - 0s - 2ms/step - loss: 0.5194 - val_loss: 0.5143\nEpoch 6/200\n183/183 - 0s - 2ms/step - loss: 0.4680 - val_loss: 0.4675\nEpoch 7/200\n183/183 - 0s - 2ms/step - loss: 0.4298 - val_loss: 0.4332\nEpoch 8/200\n183/183 - 0s - 2ms/step - loss: 0.4012 - val_loss: 0.4059\nEpoch 9/200\n183/183 - 0s - 2ms/step - loss: 0.3786 - val_loss: 0.3870\nEpoch 10/200\n183/183 - 0s - 2ms/step - loss: 0.3608 - val_loss: 0.3708\nEpoch 11/200\n183/183 - 0s - 2ms/step - loss: 0.3465 - val_loss: 0.3582\nEpoch 12/200\n183/183 - 0s - 2ms/step - loss: 0.3341 - val_loss: 0.3464\nEpoch 13/200\n183/183 - 0s - 2ms/step - loss: 0.3238 - val_loss: 0.3372\nEpoch 14/200\n183/183 - 0s - 2ms/step - loss: 0.3148 - val_loss: 0.3295\nEpoch 15/200\n183/183 - 0s - 2ms/step - loss: 0.3071 - val_loss: 0.3227\nEpoch 16/200\n183/183 - 0s - 2ms/step - loss: 0.3001 - val_loss: 0.3167\nEpoch 17/200\n183/183 - 0s - 2ms/step - loss: 0.2937 - val_loss: 0.3111\nEpoch 18/200\n183/183 - 0s - 2ms/step - loss: 0.2884 - val_loss: 0.3060\nEpoch 19/200\n183/183 - 0s - 2ms/step - loss: 0.2837 - val_loss: 0.3019\nEpoch 20/200\n183/183 - 0s - 2ms/step - loss: 0.2802 - val_loss: 0.3002\nEpoch 21/200\n183/183 - 0s - 2ms/step - loss: 0.2778 - val_loss: 0.2960\nEpoch 22/200\n183/183 - 0s - 2ms/step - loss: 0.2754 - val_loss: 0.2939\nEpoch 23/200\n183/183 - 0s - 2ms/step - loss: 0.2742 - val_loss: 0.2936\nEpoch 24/200\n183/183 - 0s - 2ms/step - loss: 0.2725 - val_loss: 0.2920\nEpoch 25/200\n183/183 - 0s - 2ms/step - loss: 0.2714 - val_loss: 0.2916\nEpoch 26/200\n183/183 - 0s - 2ms/step - loss: 0.2706 - val_loss: 0.2923\nEpoch 27/200\n183/183 - 0s - 2ms/step - loss: 0.2697 - val_loss: 0.2893\nEpoch 28/200\n183/183 - 0s - 2ms/step - loss: 0.2686 - val_loss: 0.2899\nEpoch 29/200\n183/183 - 0s - 2ms/step - loss: 0.2684 - val_loss: 0.2888\nEpoch 30/200\n183/183 - 0s - 2ms/step - loss: 0.2676 - val_loss: 0.2890\nEpoch 31/200\n183/183 - 0s - 2ms/step - loss: 0.2673 - val_loss: 0.2880\nEpoch 32/200\n183/183 - 0s - 2ms/step - loss: 0.2669 - val_loss: 0.2876\nEpoch 33/200\n183/183 - 0s - 2ms/step - loss: 0.2665 - val_loss: 0.2863\nEpoch 34/200\n183/183 - 0s - 2ms/step - loss: 0.2658 - val_loss: 0.2859\nEpoch 35/200\n183/183 - 0s - 2ms/step - loss: 0.2657 - val_loss: 0.2872\nEpoch 36/200\n183/183 - 0s - 2ms/step - loss: 0.2655 - val_loss: 0.2863\nEpoch 37/200\n183/183 - 0s - 2ms/step - loss: 0.2651 - val_loss: 0.2862\nEpoch 38/200\n183/183 - 0s - 2ms/step - loss: 0.2649 - val_loss: 0.2867\nEpoch 39/200\n183/183 - 0s - 2ms/step - loss: 0.2647 - val_loss: 0.2862\nEpoch 40/200\n183/183 - 0s - 2ms/step - loss: 0.2641 - val_loss: 0.2846\nEpoch 41/200\n183/183 - 0s - 2ms/step - loss: 0.2629 - val_loss: 0.2835\nEpoch 42/200\n183/183 - 0s - 2ms/step - loss: 0.2623 - val_loss: 0.2831\nEpoch 43/200\n183/183 - 0s - 2ms/step - loss: 0.2621 - val_loss: 0.2847\nEpoch 44/200\n183/183 - 0s - 2ms/step - loss: 0.2619 - val_loss: 0.2844\nEpoch 45/200\n183/183 - 0s - 2ms/step - loss: 0.2614 - val_loss: 0.2833\nEpoch 46/200\n183/183 - 0s - 2ms/step - loss: 0.2614 - val_loss: 0.2829\nEpoch 47/200\n183/183 - 0s - 2ms/step - loss: 0.2614 - val_loss: 0.2830\nEpoch 48/200\n183/183 - 0s - 2ms/step - loss: 0.2615 - val_loss: 0.2837\nEpoch 49/200\n183/183 - 0s - 2ms/step - loss: 0.2607 - val_loss: 0.2845\nEpoch 50/200\n183/183 - 0s - 2ms/step - loss: 0.2609 - val_loss: 0.2832\nEpoch 51/200\n183/183 - 0s - 2ms/step - loss: 0.2607 - val_loss: 0.2842\nEpoch 52/200\n183/183 - 0s - 2ms/step - loss: 0.2606 - val_loss: 0.2853\nEpoch 53/200\n183/183 - 0s - 2ms/step - loss: 0.2605 - val_loss: 0.2837\nEpoch 54/200\n183/183 - 0s - 2ms/step - loss: 0.2606 - val_loss: 0.2838\nEpoch 55/200\n183/183 - 0s - 2ms/step - loss: 0.2601 - val_loss: 0.2845\nEpoch 56/200\n183/183 - 0s - 2ms/step - loss: 0.2603 - val_loss: 0.2837\n\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n              precision    recall  f1-score   support\n\n           0       0.90      0.93      0.92       640\n           1       0.94      0.91      0.92       689\n\n    accuracy                           0.92      1329\n   macro avg       0.92      0.92      0.92      1329\nweighted avg       0.92      0.92      0.92      1329\n\nFinal Count of 0s (No Heart Attack) in new training set: 3156\nFinal Count of 1s (Heart Attack) in new training set: 3489\n","output_type":"stream"}]},{"cell_type":"code","source":"model =ExtraTreesClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       640\n           1       0.95      0.94      0.94       689\n\n    accuracy                           0.94      1329\n   macro avg       0.94      0.94      0.94      1329\nweighted avg       0.94      0.94      0.94      1329\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# gboost_model = GradientBoostingClassifier(random_state=42)\n# gboost_model.fit(x_train, y_train)\n\n# y_gboost_pred = gboost_model.predict(x_test)\n# print(classification_report(y_test, y_gboost_pred))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:47.260329Z","iopub.status.idle":"2024-10-25T20:18:47.260735Z","shell.execute_reply.started":"2024-10-25T20:18:47.260534Z","shell.execute_reply":"2024-10-25T20:18:47.260555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n# xgb_model.fit(x_train, y_train)\n\n# y_xgb_pred = xgb_model.predict(x_test)\n# print(classification_report(y_test, y_xgb_pred))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:47.262981Z","iopub.status.idle":"2024-10-25T20:18:47.263572Z","shell.execute_reply.started":"2024-10-25T20:18:47.263271Z","shell.execute_reply":"2024-10-25T20:18:47.263313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import statsmodels.api as sm\n\n# imp = sm.MICEData(data)\n# data = imp.data\n# data\n\n\n\n# target = data['TenYearCHD']\n# columns_to_drop = ['TenYearCHD']\n# features = data.drop(columns_to_drop, axis=1)\n# X = features.values\n# y = target.values\n# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# base_learners = [\n#     ('dt', DecisionTreeClassifier()), \n#     ('knn', KNeighborsClassifier()),\n#     ('et', ExtraTreesClassifier()),  # Extra Trees Classifier\n#     ('rf', RandomForestClassifier()),  # Random Forest Classifier\n#     ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))  # XGBoost\n# ]\n\n# meta_model = RandomForestClassifier()\n# stack_model = StackingClassifier(estimators=base_learners, final_estimator=meta_model)\n# stack_model.fit(x_train, y_train)\n\n# y_pred = stack_model.predict(x_test)\n# accuracy = accuracy_score(y_test, y_pred)\n\n# print(f\"Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:18:47.265548Z","iopub.status.idle":"2024-10-25T20:18:47.266133Z","shell.execute_reply.started":"2024-10-25T20:18:47.265809Z","shell.execute_reply":"2024-10-25T20:18:47.265837Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
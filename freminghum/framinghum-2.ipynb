{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9365965,"sourceType":"datasetVersion","datasetId":5679557}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport plotly.express as px\n%matplotlib inline\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report)\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier,\n                              GradientBoostingClassifier, BaggingClassifier, HistGradientBoostingClassifier, StackingClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-16T14:29:18.516983Z","iopub.execute_input":"2024-09-16T14:29:18.517953Z","iopub.status.idle":"2024-09-16T14:29:18.536018Z","shell.execute_reply.started":"2024-09-16T14:29:18.517895Z","shell.execute_reply":"2024-09-16T14:29:18.535024Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/framingham-balanced/archive/CHD_preprocessed.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:18.538360Z","iopub.execute_input":"2024-09-16T14:29:18.538991Z","iopub.status.idle":"2024-09-16T14:29:18.590905Z","shell.execute_reply.started":"2024-09-16T14:29:18.538937Z","shell.execute_reply":"2024-09-16T14:29:18.589786Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n0        1   39          1              0         0.0     0.0   \n1        0   46          0              0         0.0     0.0   \n2        1   48          0              1        20.0     0.0   \n3        0   61          1              1        30.0     0.0   \n4        0   46          1              1        23.0     0.0   \n...    ...  ...        ...            ...         ...     ...   \n4128     1   50          0              1         1.0     0.0   \n4129     1   51          1              1        43.0     0.0   \n4130     0   48          0              1        20.0     0.0   \n4131     0   44          0              1        15.0     0.0   \n4132     0   52          0              0         0.0     0.0   \n\n      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n0                   0             0         0    195.0  106.0   70.0  26.97   \n1                   0             0         0    250.0  121.0   81.0  28.73   \n2                   0             0         0    245.0  127.5   80.0  25.34   \n3                   0             1         0    225.0  150.0   95.0  28.58   \n4                   0             0         0    285.0  130.0   84.0  23.10   \n...               ...           ...       ...      ...    ...    ...    ...   \n4128                0             1         0    313.0  179.0   92.0  25.97   \n4129                0             0         0    207.0  126.5   80.0  19.71   \n4130                0             0         0    248.0  131.0   72.0  22.00   \n4131                0             0         0    210.0  126.5   87.0  19.16   \n4132                0             0         0    269.0  133.5   83.0  21.47   \n\n      heartRate  glucose  TenYearCHD  \n0          80.0     77.0           0  \n1          95.0     76.0           0  \n2          75.0     70.0           0  \n3          65.0    103.0           1  \n4          85.0     85.0           0  \n...         ...      ...         ...  \n4128       66.0     86.0           1  \n4129       65.0     68.0           0  \n4130       84.0     86.0           0  \n4131       86.0     82.0           0  \n4132       80.0    107.0           0  \n\n[4133 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n      <th>TenYearCHD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>48</td>\n      <td>0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>46</td>\n      <td>1</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4128</th>\n      <td>1</td>\n      <td>50</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>313.0</td>\n      <td>179.0</td>\n      <td>92.0</td>\n      <td>25.97</td>\n      <td>66.0</td>\n      <td>86.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4129</th>\n      <td>1</td>\n      <td>51</td>\n      <td>1</td>\n      <td>1</td>\n      <td>43.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>207.0</td>\n      <td>126.5</td>\n      <td>80.0</td>\n      <td>19.71</td>\n      <td>65.0</td>\n      <td>68.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4130</th>\n      <td>0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248.0</td>\n      <td>131.0</td>\n      <td>72.0</td>\n      <td>22.00</td>\n      <td>84.0</td>\n      <td>86.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4131</th>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>210.0</td>\n      <td>126.5</td>\n      <td>87.0</td>\n      <td>19.16</td>\n      <td>86.0</td>\n      <td>82.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4132</th>\n      <td>0</td>\n      <td>52</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>269.0</td>\n      <td>133.5</td>\n      <td>83.0</td>\n      <td>21.47</td>\n      <td>80.0</td>\n      <td>107.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4133 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:18.592426Z","iopub.execute_input":"2024-09-16T14:29:18.592826Z","iopub.status.idle":"2024-09-16T14:29:18.606076Z","shell.execute_reply.started":"2024-09-16T14:29:18.592787Z","shell.execute_reply":"2024-09-16T14:29:18.604927Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4133 entries, 0 to 4132\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   male             4133 non-null   int64  \n 1   age              4133 non-null   int64  \n 2   education        4133 non-null   int64  \n 3   currentSmoker    4133 non-null   int64  \n 4   cigsPerDay       4133 non-null   float64\n 5   BPMeds           4133 non-null   float64\n 6   prevalentStroke  4133 non-null   int64  \n 7   prevalentHyp     4133 non-null   int64  \n 8   diabetes         4133 non-null   int64  \n 9   totChol          4133 non-null   float64\n 10  sysBP            4133 non-null   float64\n 11  diaBP            4133 non-null   float64\n 12  BMI              4133 non-null   float64\n 13  heartRate        4133 non-null   float64\n 14  glucose          4133 non-null   float64\n 15  TenYearCHD       4133 non-null   int64  \ndtypes: float64(8), int64(8)\nmemory usage: 516.8 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df= data.iloc[:4020]\ncount_0=df['TenYearCHD'].eq(0).sum()\ncount_1=df['TenYearCHD'].eq(1).sum()\n\nprint(\"count 0\",count_0)\nprint(\"count 1\",count_1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:18.609184Z","iopub.execute_input":"2024-09-16T14:29:18.609788Z","iopub.status.idle":"2024-09-16T14:29:18.620566Z","shell.execute_reply.started":"2024-09-16T14:29:18.609736Z","shell.execute_reply":"2024-09-16T14:29:18.619371Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"count 0 3412\ncount 1 608\n","output_type":"stream"}]},{"cell_type":"code","source":"target= data['TenYearCHD']\ncolumns_to_drop = ['TenYearCHD']\nfeatures = data.drop(columns_to_drop, axis=1)\nfeatures","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:18.621776Z","iopub.execute_input":"2024-09-16T14:29:18.622169Z","iopub.status.idle":"2024-09-16T14:29:18.656198Z","shell.execute_reply.started":"2024-09-16T14:29:18.622132Z","shell.execute_reply":"2024-09-16T14:29:18.654926Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n0        1   39          1              0         0.0     0.0   \n1        0   46          0              0         0.0     0.0   \n2        1   48          0              1        20.0     0.0   \n3        0   61          1              1        30.0     0.0   \n4        0   46          1              1        23.0     0.0   \n...    ...  ...        ...            ...         ...     ...   \n4128     1   50          0              1         1.0     0.0   \n4129     1   51          1              1        43.0     0.0   \n4130     0   48          0              1        20.0     0.0   \n4131     0   44          0              1        15.0     0.0   \n4132     0   52          0              0         0.0     0.0   \n\n      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n0                   0             0         0    195.0  106.0   70.0  26.97   \n1                   0             0         0    250.0  121.0   81.0  28.73   \n2                   0             0         0    245.0  127.5   80.0  25.34   \n3                   0             1         0    225.0  150.0   95.0  28.58   \n4                   0             0         0    285.0  130.0   84.0  23.10   \n...               ...           ...       ...      ...    ...    ...    ...   \n4128                0             1         0    313.0  179.0   92.0  25.97   \n4129                0             0         0    207.0  126.5   80.0  19.71   \n4130                0             0         0    248.0  131.0   72.0  22.00   \n4131                0             0         0    210.0  126.5   87.0  19.16   \n4132                0             0         0    269.0  133.5   83.0  21.47   \n\n      heartRate  glucose  \n0          80.0     77.0  \n1          95.0     76.0  \n2          75.0     70.0  \n3          65.0    103.0  \n4          85.0     85.0  \n...         ...      ...  \n4128       66.0     86.0  \n4129       65.0     68.0  \n4130       84.0     86.0  \n4131       86.0     82.0  \n4132       80.0    107.0  \n\n[4133 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>48</td>\n      <td>0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>46</td>\n      <td>1</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4128</th>\n      <td>1</td>\n      <td>50</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>313.0</td>\n      <td>179.0</td>\n      <td>92.0</td>\n      <td>25.97</td>\n      <td>66.0</td>\n      <td>86.0</td>\n    </tr>\n    <tr>\n      <th>4129</th>\n      <td>1</td>\n      <td>51</td>\n      <td>1</td>\n      <td>1</td>\n      <td>43.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>207.0</td>\n      <td>126.5</td>\n      <td>80.0</td>\n      <td>19.71</td>\n      <td>65.0</td>\n      <td>68.0</td>\n    </tr>\n    <tr>\n      <th>4130</th>\n      <td>0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248.0</td>\n      <td>131.0</td>\n      <td>72.0</td>\n      <td>22.00</td>\n      <td>84.0</td>\n      <td>86.0</td>\n    </tr>\n    <tr>\n      <th>4131</th>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>210.0</td>\n      <td>126.5</td>\n      <td>87.0</td>\n      <td>19.16</td>\n      <td>86.0</td>\n      <td>82.0</td>\n    </tr>\n    <tr>\n      <th>4132</th>\n      <td>0</td>\n      <td>52</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>269.0</td>\n      <td>133.5</td>\n      <td>83.0</td>\n      <td>21.47</td>\n      <td>80.0</td>\n      <td>107.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4133 rows × 15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# All undersampling","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids, NearMiss, EditedNearestNeighbours\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\nrus = RandomUnderSampler()\nX_RUS, y_RUS = rus.fit_resample(X_train, y_train)\n\nrf_model_rus = RandomForestClassifier(random_state=42)\nrf_model_rus.fit(X_RUS, y_RUS)\ny_pred_rus = rf_model_rus.predict(X_test)\nprint(\"Random Undersampling Accuracy:\", accuracy_score(y_test, y_pred_rus))\nprint(classification_report(y_test, y_pred_rus))\n\n\ntl = TomekLinks()\nX_TL, y_TL = tl.fit_resample(X_train, y_train)\n\nrf_model_tl = RandomForestClassifier(random_state=42)\nrf_model_tl.fit(X_TL, y_TL)\ny_pred_tl = rf_model_tl.predict(X_test)\nprint(\"\\nTomek Links Accuracy:\", accuracy_score(y_test, y_pred_tl))\nprint(classification_report(y_test, y_pred_tl))\n\n\ncc = ClusterCentroids()\nX_CC, y_CC = cc.fit_resample(X_train, y_train)\n\nrf_model_cc = RandomForestClassifier(random_state=42)\nrf_model_cc.fit(X_CC, y_CC)\ny_pred_cc = rf_model_cc.predict(X_test)\nprint(\"\\nCluster Centroids Accuracy:\", accuracy_score(y_test, y_pred_cc))\nprint(classification_report(y_test, y_pred_cc))\n\n\nnm = NearMiss()\nX_NM, y_NM = nm.fit_resample(X_train, y_train)\n\nrf_model_nm = RandomForestClassifier(random_state=42)\nrf_model_nm.fit(X_NM, y_NM)\ny_pred_nm = rf_model_nm.predict(X_test)\nprint(\"\\nNearMiss Accuracy:\", accuracy_score(y_test, y_pred_nm))\nprint(classification_report(y_test, y_pred_nm))\n\n\nenn = EditedNearestNeighbours()\nX_ENN, y_ENN = enn.fit_resample(X_train, y_train)\n\nrf_model_enn = RandomForestClassifier(random_state=42)\nrf_model_enn.fit(X_ENN, y_ENN)\ny_pred_enn = rf_model_enn.predict(X_test)\nprint(\"\\nEdited Nearest Neighbours Accuracy:\", accuracy_score(y_test, y_pred_enn))\nprint(classification_report(y_test, y_pred_enn))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:18.658107Z","iopub.execute_input":"2024-09-16T14:29:18.658470Z","iopub.status.idle":"2024-09-16T14:29:26.079473Z","shell.execute_reply.started":"2024-09-16T14:29:18.658431Z","shell.execute_reply":"2024-09-16T14:29:26.078411Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Random Undersampling Accuracy: 0.6481257557436517\n              precision    recall  f1-score   support\n\n           0       0.89      0.65      0.75       680\n           1       0.28      0.64      0.39       147\n\n    accuracy                           0.65       827\n   macro avg       0.59      0.64      0.57       827\nweighted avg       0.78      0.65      0.69       827\n\n\nTomek Links Accuracy: 0.8319226118500604\n              precision    recall  f1-score   support\n\n           0       0.84      0.99      0.91       680\n           1       0.70      0.10      0.17       147\n\n    accuracy                           0.83       827\n   macro avg       0.77      0.54      0.54       827\nweighted avg       0.81      0.83      0.78       827\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nCluster Centroids Accuracy: 0.3385731559854897\n              precision    recall  f1-score   support\n\n           0       0.88      0.23      0.36       680\n           1       0.19      0.85      0.31       147\n\n    accuracy                           0.34       827\n   macro avg       0.53      0.54      0.34       827\nweighted avg       0.75      0.34      0.35       827\n\n\nNearMiss Accuracy: 0.42926239419588874\n              precision    recall  f1-score   support\n\n           0       0.90      0.35      0.50       680\n           1       0.21      0.82      0.34       147\n\n    accuracy                           0.43       827\n   macro avg       0.55      0.58      0.42       827\nweighted avg       0.78      0.43      0.47       827\n\n\nEdited Nearest Neighbours Accuracy: 0.8174123337363967\n              precision    recall  f1-score   support\n\n           0       0.85      0.94      0.89       680\n           1       0.47      0.26      0.33       147\n\n    accuracy                           0.82       827\n   macro avg       0.66      0.60      0.61       827\nweighted avg       0.79      0.82      0.79       827\n\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:26.080705Z","iopub.execute_input":"2024-09-16T14:29:26.081057Z","iopub.status.idle":"2024-09-16T14:29:26.110577Z","shell.execute_reply.started":"2024-09-16T14:29:26.081021Z","shell.execute_reply":"2024-09-16T14:29:26.109397Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n173      0   60          1              0         0.0     0.0   \n1022     1   42          1              1        20.0     0.0   \n3182     1   58          0              0         0.0     0.0   \n331      0   58          0              0         0.0     0.0   \n2222     1   39          1              0         0.0     0.0   \n...    ...  ...        ...            ...         ...     ...   \n3444     0   49          0              0         0.0     0.0   \n466      1   50          0              0         0.0     0.0   \n3092     0   36          0              0         0.0     0.0   \n3772     0   64          0              0         0.0     0.0   \n860      0   47          0              0         0.0     0.0   \n\n      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n173                 0             1         0    325.0  182.0  106.0  27.61   \n1022                0             0         0    270.0  112.0   77.0  24.77   \n3182                0             0         0    225.0  105.5   74.0  25.68   \n331                 0             1         0    200.0  158.0  101.0  23.06   \n2222                0             1         0    208.0  146.0   92.0  25.91   \n...               ...           ...       ...      ...    ...    ...    ...   \n3444                0             1         0    233.0  149.0   91.5  26.03   \n466                 0             1         0    219.0  145.0  100.0  26.26   \n3092                0             0         0    209.0  107.0   73.5  21.59   \n3772                0             1         0    279.0  172.0   87.0  24.01   \n860                 0             0         0    232.0  113.5   73.0  28.78   \n\n      heartRate  glucose  \n173        80.0     77.0  \n1022       73.0     85.0  \n3182       50.0     93.0  \n331        85.0     77.0  \n2222       69.0     74.0  \n...         ...      ...  \n3444       68.0     82.0  \n466        78.0    108.0  \n3092       75.0     73.0  \n3772       80.0     70.0  \n860        75.0     77.0  \n\n[3306 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>173</th>\n      <td>0</td>\n      <td>60</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>325.0</td>\n      <td>182.0</td>\n      <td>106.0</td>\n      <td>27.61</td>\n      <td>80.0</td>\n      <td>77.0</td>\n    </tr>\n    <tr>\n      <th>1022</th>\n      <td>1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>270.0</td>\n      <td>112.0</td>\n      <td>77.0</td>\n      <td>24.77</td>\n      <td>73.0</td>\n      <td>85.0</td>\n    </tr>\n    <tr>\n      <th>3182</th>\n      <td>1</td>\n      <td>58</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>105.5</td>\n      <td>74.0</td>\n      <td>25.68</td>\n      <td>50.0</td>\n      <td>93.0</td>\n    </tr>\n    <tr>\n      <th>331</th>\n      <td>0</td>\n      <td>58</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>200.0</td>\n      <td>158.0</td>\n      <td>101.0</td>\n      <td>23.06</td>\n      <td>85.0</td>\n      <td>77.0</td>\n    </tr>\n    <tr>\n      <th>2222</th>\n      <td>1</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>208.0</td>\n      <td>146.0</td>\n      <td>92.0</td>\n      <td>25.91</td>\n      <td>69.0</td>\n      <td>74.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3444</th>\n      <td>0</td>\n      <td>49</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>233.0</td>\n      <td>149.0</td>\n      <td>91.5</td>\n      <td>26.03</td>\n      <td>68.0</td>\n      <td>82.0</td>\n    </tr>\n    <tr>\n      <th>466</th>\n      <td>1</td>\n      <td>50</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>219.0</td>\n      <td>145.0</td>\n      <td>100.0</td>\n      <td>26.26</td>\n      <td>78.0</td>\n      <td>108.0</td>\n    </tr>\n    <tr>\n      <th>3092</th>\n      <td>0</td>\n      <td>36</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>209.0</td>\n      <td>107.0</td>\n      <td>73.5</td>\n      <td>21.59</td>\n      <td>75.0</td>\n      <td>73.0</td>\n    </tr>\n    <tr>\n      <th>3772</th>\n      <td>0</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>279.0</td>\n      <td>172.0</td>\n      <td>87.0</td>\n      <td>24.01</td>\n      <td>80.0</td>\n      <td>70.0</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>0</td>\n      <td>47</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>232.0</td>\n      <td>113.5</td>\n      <td>73.0</td>\n      <td>28.78</td>\n      <td>75.0</td>\n      <td>77.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3306 rows × 15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# SMOTH","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\ntarget = data['TenYearCHD']\ncolumns_to_drop = ['TenYearCHD']\nfeatures = data.drop(columns_to_drop, axis=1)\n\nx = features.values\ny = target.values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nsmote = SMOTE(random_state=42)\nx_train, y_train = smote.fit_resample(x_train, y_train)\n\n# model = RandomForestClassifier(random_state=42)\n# model.fit(x_train, y_train)\n\n# y_pred = model.predict(x_test)\n\n# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:26.111933Z","iopub.execute_input":"2024-09-16T14:29:26.112286Z","iopub.status.idle":"2024-09-16T14:29:26.132258Z","shell.execute_reply.started":"2024-09-16T14:29:26.112250Z","shell.execute_reply":"2024-09-16T14:29:26.131119Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"RandomForest\": RandomForestClassifier(random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=1500, random_state=42),\n    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n    \"Bagging\": BaggingClassifier(random_state=42),\n    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n    \"LogisticRegression\": LogisticRegression(random_state=42),\n    \"GaussianNB\": GaussianNB(),\n    \"BernoulliNB\": BernoulliNB(),\n    \"MultinomialNB\": MultinomialNB(),\n    \"KNeighbors\": KNeighborsClassifier(),\n    #\"RadiusNeighbors\": RadiusNeighborsClassifier(),\n    \"SVC\": SVC(probability=True, random_state=42),\n    \"XGBoost\": XGBClassifier(random_state=42),\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    \"MLPClassifier\": MLPClassifier(random_state=42)\n}\n\nresults_df = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'Specificity', 'ROC-AUC', 'Confusion Matrix'])\n\n# def evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test):\n#     for name, clf in classifiers.items():\n#         print(f\"\\nClassifier: {name}\")\n#         try:\n#             clf.fit(x_train, y_train)\n#             y_pred = clf.predict(x_test)\n\n#             print(\"Classification Report:\")\n#             print(classification_report(y_test, y_pred))\n\n#             cm = confusion_matrix(y_test, y_pred)\n#             print(\"Confusion Matrix:\")\n#             print(cm)\n\n#             acc = accuracy_score(y_test, y_pred)\n#             print(f\"Accuracy: {acc:.4f}\")\n\n#             f1 = f1_score(y_test, y_pred, average='weighted')\n#             print(f\"F1 Score: {f1:.4f}\")\n\n#             if hasattr(clf, \"predict_proba\"): \n#                 y_proba = clf.predict_proba(x_test)[:, 1]\n#                 roc_auc = roc_auc_score(y_test, y_proba)\n#                 print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n#         except (ValueError, NotFittedError) as e:\n#             print(f\"Error with classifier {name}: {e}\")\n\n# evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:26.137127Z","iopub.execute_input":"2024-09-16T14:29:26.137510Z","iopub.status.idle":"2024-09-16T14:29:26.152091Z","shell.execute_reply.started":"2024-09-16T14:29:26.137472Z","shell.execute_reply":"2024-09-16T14:29:26.150674Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test):\n    global results_df\n    for name, clf in classifiers.items():\n        try:\n            clf.fit(x_train, y_train) \n            y_pred = clf.predict(x_test)  \n\n            cm = confusion_matrix(y_test, y_pred)\n            if len(cm.ravel()) == 4:  \n                tn, fp, fn, tp = cm.ravel()\n                specificity = tn / (tn + fp) \n            else:\n                specificity = None\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred, average='weighted')\n            report = classification_report(y_test, y_pred, output_dict=True)\n            precision = report['weighted avg']['precision']\n            recall = report['weighted avg']['recall']\n\n            if hasattr(clf, \"predict_proba\"): \n                y_proba = clf.predict_proba(x_test)[:, 1]\n                roc_auc = roc_auc_score(y_test, y_proba)\n            else:\n                roc_auc = None\n                \n            new_row = pd.DataFrame({\n                'Model': [name],\n                'Accuracy': [acc],\n                'F1 Score': [f1],\n                'Precision': [precision],\n                'Recall': [recall],\n                'Specificity': [specificity],\n                'ROC-AUC': [roc_auc],\n                'Confusion Matrix': [cm]\n            })\n            results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n        except (ValueError, NotFittedError) as e:\n            print(f\"Error with classifier {name}: {e}\")\n            \nevaluate_classifiers(classifiers, x_train, x_test, y_train, y_test)\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:26.153504Z","iopub.execute_input":"2024-09-16T14:29:26.153961Z","iopub.status.idle":"2024-09-16T14:29:52.935239Z","shell.execute_reply.started":"2024-09-16T14:29:26.153918Z","shell.execute_reply":"2024-09-16T14:29:52.933312Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3692562646.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results_df = pd.concat([results_df, new_row], ignore_index=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2825, number of negative: 2825\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001173 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3259\n[LightGBM] [Info] Number of data points in the train set: 5650, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n                   Model  Accuracy  F1 Score  Precision    Recall  \\\n0           RandomForest  0.816203  0.772429   0.766545  0.816203   \n1               AdaBoost  0.778718  0.767350   0.758510  0.778718   \n2             ExtraTrees  0.814994  0.764340   0.757535  0.814994   \n3       GradientBoosting  0.801693  0.767799   0.753688  0.801693   \n4                Bagging  0.814994  0.776874   0.769310  0.814994   \n5   HistGradientBoosting  0.808948  0.770226   0.758824  0.808948   \n6           DecisionTree  0.731560  0.733649   0.735803  0.731560   \n7     LogisticRegression  0.674728  0.711146   0.797872  0.674728   \n8             GaussianNB  0.771463  0.778278   0.786484  0.771463   \n9            BernoulliNB  0.702539  0.724286   0.755955  0.702539   \n10         MultinomialNB  0.620314  0.663801   0.760027  0.620314   \n11            KNeighbors  0.640871  0.679375   0.752921  0.640871   \n12                   SVC  0.652963  0.692855   0.792036  0.652963   \n13               XGBoost  0.796856  0.759639   0.742449  0.796856   \n14              LightGBM  0.812576  0.772672   0.763567  0.812576   \n15         MLPClassifier  0.425635  0.462538   0.790956  0.425635   \n\n    Specificity   ROC-AUC         Confusion Matrix  \n0      0.964706  0.676346   [[656, 24], [128, 19]]  \n1      0.889706  0.695488   [[605, 75], [108, 39]]  \n2      0.970588  0.677141   [[660, 20], [133, 14]]  \n3      0.941176  0.692682   [[640, 40], [124, 23]]  \n4      0.957353  0.629052   [[651, 29], [124, 23]]  \n5      0.952941  0.639036   [[648, 32], [126, 21]]  \n6      0.832353  0.548830  [[566, 114], [108, 39]]  \n7      0.676471  0.705732   [[460, 220], [49, 98]]  \n8      0.842647  0.715916   [[573, 107], [82, 65]]  \n9      0.763235  0.655267   [[519, 161], [85, 62]]  \n10     0.632353  0.644678   [[430, 250], [64, 83]]  \n11     0.670588  0.607333   [[456, 224], [73, 74]]  \n12     0.650000  0.717767   [[442, 238], [49, 98]]  \n13     0.941176  0.614056   [[640, 40], [128, 19]]  \n14     0.957353  0.630592   [[651, 29], [126, 21]]  \n15     0.332353  0.696949  [[226, 454], [21, 126]]  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TomekLinks SMOTH","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import TomekLinks\n\ntarget = data['TenYearCHD']\ncolumns_to_drop = ['TenYearCHD']\nfeatures = data.drop(columns_to_drop, axis=1)\n\nx= features.values\ny= target.values\n\ntomeklinks= TomekLinks()\noversample= SMOTE()\nsteps= [(\"TL\", tomeklinks), (\"os\", oversample)]\npipeline= Pipeline(steps=steps)\n\nx_s, y_s = pipeline.fit_resample(x, y)\n\nx_train, x_test,y_train, y_test = train_test_split(x_s, y_s, test_size=0.2, random_state=42)\n\n# model= RandomForestClassifier(random_state=42)\n# model.fit(x_train,y_train)\n\n# y_pred = model.predict(x_test)\n# print(\"Classification Report:\\n\",classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:52.937028Z","iopub.execute_input":"2024-09-16T14:29:52.938288Z","iopub.status.idle":"2024-09-16T14:29:53.102566Z","shell.execute_reply.started":"2024-09-16T14:29:52.938214Z","shell.execute_reply":"2024-09-16T14:29:53.101498Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"y_counts = pd.Series(y_s).value_counts()\nprint(f\"Count of 0: {y_counts[0]}\")\nprint(f\"Count of 1: {y_counts[1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:53.104055Z","iopub.execute_input":"2024-09-16T14:29:53.104419Z","iopub.status.idle":"2024-09-16T14:29:53.111592Z","shell.execute_reply.started":"2024-09-16T14:29:53.104381Z","shell.execute_reply":"2024-09-16T14:29:53.110376Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Count of 0: 3299\nCount of 1: 3299\n","output_type":"stream"}]},{"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n# from imblearn.pipeline import Pipeline\n# from imblearn.under_sampling import TomekLinks\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import train_test_split, GridSearchCV\n# from sklearn.metrics import classification_report, accuracy_score\n\n# # Define features and target arrays\n# x = features.values\n# y = target.values\n\n# # Initialize Tomek Links and SMOTE with a more balanced sampling strategy\n# tomeklinks = TomekLinks()\n# smote = SMOTE(sampling_strategy=0.5)  # Reduce the amount of oversampling\n\n# # Create a pipeline with Tomek Links and SMOTE\n# steps = [\n#     (\"TL\", tomeklinks),  # Remove noisy/borderline examples\n#     (\"smote\", smote)     # Generate synthetic samples with SMOTE\n# ]\n# pipeline = Pipeline(steps=steps)\n\n# # Apply the pipeline to resample the dataset\n# x_s, y_s = pipeline.fit_resample(x, y)\n\n# # Split the resampled data into training and testing sets\n# x_train, x_test, y_train, y_test = train_test_split(x_s, y_s, test_size=0.2, random_state=42)\n\n# # Initialize a classifier (e.g., RandomForest) and use GridSearchCV for hyperparameter tuning\n# param_grid = {\n#     'n_estimators': [100, 200, 300],\n#     'max_depth': [5, 10, 20],\n#     'min_samples_split': [2, 5, 10],\n#     'min_samples_leaf': [1, 2, 4]\n# }\n\n# rf = RandomForestClassifier(random_state=42)\n# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# # Fit the model with the best parameters\n# grid_search.fit(x_train, y_train)\n# best_model = grid_search.best_estimator_\n\n# # Predict on the test set\n# y_pred = best_model.predict(x_test)\n\n# # Evaluate the model performance\n# print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:53.113386Z","iopub.execute_input":"2024-09-16T14:29:53.114390Z","iopub.status.idle":"2024-09-16T14:29:53.125622Z","shell.execute_reply.started":"2024-09-16T14:29:53.114337Z","shell.execute_reply":"2024-09-16T14:29:53.124488Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"RandomForest\": RandomForestClassifier(random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n    \"ExtraTrees\": ExtraTreesClassifier(random_state=42),\n    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n    \"Bagging\": BaggingClassifier(random_state=42),\n    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n    \"LogisticRegression\": LogisticRegression(random_state=42),\n    \"GaussianNB\": GaussianNB(),\n    \"BernoulliNB\": BernoulliNB(),\n    #\"MultinomialNB\": MultinomialNB(),\n    \"KNeighbors\": KNeighborsClassifier(),\n    #\"RadiusNeighbors\": RadiusNeighborsClassifier(),\n    \"SVC\": SVC(probability=True, random_state=42),\n    \"XGBoost\": XGBClassifier(random_state=42),\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    \"MLPClassifier\": MLPClassifier(random_state=42)\n}\nresults_df = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'Specificity', 'ROC-AUC', 'Confusion Matrix'])\n\n\ndef evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test):\n    global results_df\n    for name, clf in classifiers.items():\n        try:\n            clf.fit(x_train, y_train) \n            y_pred = clf.predict(x_test)  \n\n            cm = confusion_matrix(y_test, y_pred)\n            if len(cm.ravel()) == 4:  \n                tn, fp, fn, tp = cm.ravel()\n                specificity = tn / (tn + fp) \n            else:\n                specificity = None\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred, average='weighted')\n            report = classification_report(y_test, y_pred, output_dict=True)\n            precision = report['weighted avg']['precision']\n            recall = report['weighted avg']['recall']\n\n            if hasattr(clf, \"predict_proba\"): \n                y_proba = clf.predict_proba(x_test)[:, 1]\n                roc_auc = roc_auc_score(y_test, y_proba)\n            else:\n                roc_auc = None\n                \n            new_row = pd.DataFrame({\n                'Model': [name],\n                'Accuracy': [acc],\n                'F1 Score': [f1],\n                'Precision': [precision],\n                'Recall': [recall],\n                'Specificity': [specificity],\n                'ROC-AUC': [roc_auc],\n                'Confusion Matrix': [cm]\n            })\n            results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n        except (ValueError, NotFittedError) as e:\n            print(f\"Error with classifier {name}: {e}\")\n            \nevaluate_classifiers(classifiers, x_train, x_test, y_train, y_test)\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:29:53.127137Z","iopub.execute_input":"2024-09-16T14:29:53.127495Z","iopub.status.idle":"2024-09-16T14:30:08.123232Z","shell.execute_reply.started":"2024-09-16T14:29:53.127459Z","shell.execute_reply":"2024-09-16T14:30:08.122052Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1245361844.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results_df = pd.concat([results_df, new_row], ignore_index=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2660, number of negative: 2618\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3243\n[LightGBM] [Info] Number of data points in the train set: 5278, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503979 -> initscore=0.015915\n[LightGBM] [Info] Start training from score 0.015915\n                   Model  Accuracy  F1 Score  Precision    Recall  \\\n0           RandomForest  0.915909  0.915779   0.916944  0.915909   \n1               AdaBoost  0.807576  0.807096   0.808770  0.807576   \n2             ExtraTrees  0.937121  0.937067   0.937584  0.937121   \n3       GradientBoosting  0.871212  0.870186   0.879043  0.871212   \n4                Bagging  0.893939  0.893658   0.895968  0.893939   \n5   HistGradientBoosting  0.893939  0.893640   0.896146  0.893939   \n6           DecisionTree  0.837121  0.837149   0.838224  0.837121   \n7     LogisticRegression  0.662879  0.662945   0.663742  0.662879   \n8             GaussianNB  0.636364  0.608876   0.675133  0.636364   \n9            BernoulliNB  0.664394  0.663423   0.669400  0.664394   \n10            KNeighbors  0.826515  0.823504   0.858900  0.826515   \n11                   SVC  0.665909  0.665843   0.667616  0.665909   \n12               XGBoost  0.896212  0.895962   0.898005  0.896212   \n13              LightGBM  0.899242  0.898896   0.902190  0.899242   \n14         MLPClassifier  0.678030  0.678076   0.678159  0.678030   \n\n    Specificity   ROC-AUC          Confusion Matrix  \n0      0.944200  0.972330    [[643, 38], [73, 566]]  \n1      0.848752  0.899793  [[578, 103], [151, 488]]  \n2      0.955947  0.981295    [[651, 30], [53, 586]]  \n3      0.947137  0.934661   [[645, 36], [134, 505]]  \n4      0.933921  0.949422    [[636, 45], [95, 544]]  \n5      0.935389  0.955527    [[637, 44], [96, 543]]  \n6      0.817915  0.837752   [[557, 124], [91, 548]]  \n7      0.650514  0.708562  [[443, 238], [207, 432]]  \n8      0.888399  0.725165   [[605, 76], [404, 235]]  \n9      0.604993  0.762051  [[412, 269], [174, 465]]  \n10     0.688693  0.938477   [[469, 212], [17, 622]]  \n11     0.640235  0.732847  [[436, 245], [196, 443]]  \n12     0.933921  0.949763    [[636, 45], [92, 547]]  \n13     0.945668  0.954044    [[644, 37], [96, 543]]  \n14     0.682819  0.738174  [[465, 216], [209, 430]]  \n","output_type":"stream"}]},{"cell_type":"code","source":"exc= ExtraTreesClassifier(criterion='entropy', n_estimators= 100, max_depth= 500, min_samples_split= 3, max_features='log2')\nexc.fit(x_train,y_train)\ny_pred= exc.predict(x_test)\nprint(\"classification report: \\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:48.932411Z","iopub.execute_input":"2024-09-16T14:30:48.933436Z","iopub.status.idle":"2024-09-16T14:30:49.692010Z","shell.execute_reply.started":"2024-09-16T14:30:48.933386Z","shell.execute_reply":"2024-09-16T14:30:49.690875Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"classification report: \n               precision    recall  f1-score   support\n\n           0       0.92      0.96      0.94       681\n           1       0.95      0.92      0.93       639\n\n    accuracy                           0.94      1320\n   macro avg       0.94      0.94      0.94      1320\nweighted avg       0.94      0.94      0.94      1320\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# mlp","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(x_train.shape[1],)),  \n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),   \n    Dense(1, activation='sigmoid')                                \n])\n\noptimizer = Adam(learning_rate=0.02)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(x_test_scaled, y_test))\ntest_loss, test_accuracy = model.evaluate(x_test_scaled, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:10.480084Z","iopub.execute_input":"2024-09-16T14:30:10.480547Z","iopub.status.idle":"2024-09-16T14:30:24.105598Z","shell.execute_reply.started":"2024-09-16T14:30:10.480495Z","shell.execute_reply":"2024-09-16T14:30:24.101950Z"},"trusted":true},"execution_count":60,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m<frozen importlib._bootstrap>:188\u001b[0m, in \u001b[0;36m_get_module_lock\u001b[0;34m(name)\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'tensorflow._api.v2.compat.v1.xla'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/__init__.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py:80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m user_ops\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xla\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_array_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_to_space_nd \u001b[38;5;66;03m# line: 343\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_array_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitcast \u001b[38;5;66;03m# line: 558\u001b[39;00m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1024\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:170\u001b[0m, in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:188\u001b[0m, in \u001b[0;36m_get_module_lock\u001b[0;34m(name)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Stacked Ensemble (SVM, DT, XGBoost, LR as meta-classifier)","metadata":{}},{"cell_type":"code","source":"base_estimators = [\n    ('svm', SVC(probability=True, kernel='linear', random_state=42)),\n    #('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n    ('dt', DecisionTreeClassifier(random_state=42)),                   \n    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)) \n]\nmeta_learner = LogisticRegression()\nstacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=meta_learner, cv=5)\nstacking_clf.fit(x_train, y_train)\n\ny_pred = stacking_clf.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.106745Z","iopub.status.idle":"2024-09-16T14:30:24.107223Z","shell.execute_reply.started":"2024-09-16T14:30:24.107002Z","shell.execute_reply":"2024-09-16T14:30:24.107025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_estimators = [\n    ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n]\n\n# মেটা-মডেল (স্ট্যাকার) হিসেবে LogisticRegression\nstacking_clf = StackingClassifier(\n    estimators=base_estimators, \n    final_estimator=LogisticRegression(),\n    cv=5\n)\n\nmodel_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')), \n    ('scaler', StandardScaler()),               \n    ('stacking', stacking_clf)                   \n])\nmodel_pipeline.fit(x_train, y_train)\ny_pred = model_pipeline.predict(x_test)\n\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.109053Z","iopub.status.idle":"2024-09-16T14:30:24.109502Z","shell.execute_reply.started":"2024-09-16T14:30:24.109285Z","shell.execute_reply":"2024-09-16T14:30:24.109308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = model.predict(x_test)\n# print(\"Classification Report:\\n\",classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.111292Z","iopub.status.idle":"2024-09-16T14:30:24.111769Z","shell.execute_reply.started":"2024-09-16T14:30:24.111530Z","shell.execute_reply":"2024-09-16T14:30:24.111553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacked Sparse Autoencoder (SSAE) + PSO built this model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.constraints import max_norm\n\nx = features.values\ny = target.values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\ndef build_autoencoder(input_dim, encoding_dim, sparsity_weight=0.01):\n    # Input Layer\n    input_layer = Input(shape=(input_dim,))\n    \n    # Encoding Layers\n    encoded = Dense(encoding_dim, activation='relu', activity_regularizer=tf.keras.regularizers.l1(sparsity_weight))(input_layer)\n    \n    # Decoding Layers\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    \n    # Autoencoder Model\n    autoencoder = Model(input_layer, decoded)\n    \n    # Encoder Model\n    encoder = Model(input_layer, encoded)\n    \n    return autoencoder, encoder\n\ninput_dim = x_train_scaled.shape[1]\nencoding_dims = [128, 64]  # Example dimensions for stacking\n\n# Build Stacked Autoencoder\nautoencoder, encoder = build_autoencoder(input_dim, encoding_dims[0])\n\n# Compile the model\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Train the model\nautoencoder.fit(x_train_scaled, x_train_scaled, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test_scaled, x_test_scaled))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.113331Z","iopub.status.idle":"2024-09-16T14:30:24.113752Z","shell.execute_reply.started":"2024-09-16T14:30:24.113540Z","shell.execute_reply":"2024-09-16T14:30:24.113562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pyswarm","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.115988Z","iopub.status.idle":"2024-09-16T14:30:24.116405Z","shell.execute_reply.started":"2024-09-16T14:30:24.116201Z","shell.execute_reply":"2024-09-16T14:30:24.116222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyswarm import pso\nimport numpy as np\n\ndef objective_function(params):\n    encoding_dim1, encoding_dim2 = int(params[0]), int(params[1])\n    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim1)\n    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    \n    history = autoencoder.fit(x_train_scaled, x_train_scaled, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test_scaled, x_test_scaled), verbose=0)\n    \n    val_loss = history.history['val_loss'][-1]\n    return val_loss\n\nlb = [10, 10]  # Lower bounds for encoding dimensions\nub = [256, 128]  # Upper bounds for encoding dimensions\n\nbest_params, _ = pso(objective_function, lb, ub, swarmsize=10, maxiter=5)\nprint(\"Best Parameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.117684Z","iopub.status.idle":"2024-09-16T14:30:24.118147Z","shell.execute_reply.started":"2024-09-16T14:30:24.117923Z","shell.execute_reply":"2024-09-16T14:30:24.117946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom pyswarm import pso\n\n\ntarget = data['TenYearCHD']\ncolumns_to_drop = ['TenYearCHD']\nfeatures = data.drop(columns_to_drop, axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\ndef build_autoencoder(input_dim, encoding_dim1, encoding_dim2, sparsity_weight=0.01):\n    input_layer = Input(shape=(input_dim,))\n    encoded1 = Dense(encoding_dim1, activation='relu', activity_regularizer=tf.keras.regularizers.l1(sparsity_weight))(input_layer)\n    encoded2 = Dense(encoding_dim2, activation='relu', activity_regularizer=tf.keras.regularizers.l1(sparsity_weight))(encoded1)\n    decoded = Dense(input_dim, activation='sigmoid')(encoded2)\n    autoencoder = Model(input_layer, decoded)\n    encoder = Model(input_layer, encoded2)\n    return autoencoder, encoder\n\nencoding_dim1 = 72\nencoding_dim2 = 128\n\ninput_dim = x_train_scaled.shape[1]\nautoencoder, encoder = build_autoencoder(input_dim, encoding_dim1, encoding_dim2)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nautoencoder.fit(x_train_scaled, x_train_scaled, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test_scaled, x_test_scaled))\n\nreconstructed = autoencoder.predict(x_test_scaled)\ntest_loss = autoencoder.evaluate(x_test_scaled, x_test_scaled)\nprint(f\"Test Loss: {test_loss:.4f}\")\n\nencoded_data = encoder.predict(x_test_scaled)\nprint(f\"Encoded Data Shape: {encoded_data.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.120108Z","iopub.status.idle":"2024-09-16T14:30:24.120516Z","shell.execute_reply.started":"2024-09-16T14:30:24.120315Z","shell.execute_reply":"2024-09-16T14:30:24.120336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Borderline-SMOTE + TPE + LightGBM ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import BorderlineSMOTE\nfrom lightgbm import LGBMClassifier\n\nx = features.values\ny = target.values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\nborderline_smote = BorderlineSMOTE()\nlgbm_model = LGBMClassifier()\n\npipeline = Pipeline([\n    ('smote', borderline_smote),\n    ('lgbm', lgbm_model)\n])\n\nspace = {\n    'lgbm__num_leaves': hp.choice('num_leaves', [31, 63, 127, 255]),\n    'lgbm__learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'lgbm__n_estimators': hp.choice('n_estimators', [50, 100, 200, 300])\n}\n\ndef objective(params):\n    pipeline.set_params(**params)\n    pipeline.fit(x_train_scaled, y_train)\n    score = pipeline.score(x_test_scaled, y_test)\n    return {'loss': -score, 'status': 'ok'}\n\ntrials = Trials()\nbest = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n\nnum_leaves_options = [31, 63, 127, 255]\nn_estimators_options = [50, 100, 200, 300]\n\nbest_params = {\n    'lgbm__num_leaves': num_leaves_options[best['num_leaves']],  \n    'lgbm__learning_rate': best['learning_rate'], \n    'lgbm__n_estimators': n_estimators_options[best['n_estimators']] \n}\n\nprint(\"Best parameters found: \", best_params)\n\npipeline.set_params(**best_params)\npipeline.fit(x_train_scaled, y_train)\n\ntest_score = pipeline.score(x_test_scaled, y_test)\nprint(f\"Test Accuracy with best parameters: {test_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.122644Z","iopub.status.idle":"2024-09-16T14:30:24.123105Z","shell.execute_reply.started":"2024-09-16T14:30:24.122870Z","shell.execute_reply":"2024-09-16T14:30:24.122896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Borderline-tome-SMOTE + TPE + LightGBM  built this model","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport optuna\nimport numpy as np\nimport pandas as pd\n\n# Sample Data Loading (Replace with your own data loading code)\n# features = pd.read_csv('your_features.csv')\n# target = pd.read_csv('your_target.csv')\n\n# For demonstration, we use random data\nnp.random.seed(42)\nfeatures = pd.DataFrame(np.random.rand(1000, 20), columns=[f'feature_{i}' for i in range(20)])\ntarget = pd.Series(np.random.randint(0, 2, size=1000), name='target')\n\n# Prepare data\nx = features.values\ny = target.values\n\n# Split the dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Define preprocessing pipeline with TomekLinks and SMOTE\ntomeklinks = TomekLinks()\nsmote = SMOTE()\npipeline = Pipeline(steps=[(\"TL\", tomeklinks), (\"SMOTE\", smote)])\n\n# Apply the pipeline to the training data\nx_train_resampled, y_train_resampled = pipeline.fit_resample(x_train, y_train)\n\n# Define the objective function for Optuna\ndef objective(trial):\n    param = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n        'num_leaves': trial.suggest_int('num_leaves', 31, 255),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', -1, 50)\n    }\n    \n    # Train LightGBM model\n    model = lgb.LGBMClassifier(**param)\n    model.fit(x_train_resampled, y_train_resampled)\n    \n    # Evaluate the model\n    y_pred = model.predict(x_test)\n    accuracy = (y_pred == y_test).mean()\n    return accuracy\n\n# Create Optuna study object\nstudy = optuna.create_study(direction='maximize')\n\n# Optimize the objective function\nstudy.optimize(objective, n_trials=50)\n\n# Print the best hyperparameters\nprint(\"Best hyperparameters: \", study.best_params)\n\n# Train the final model with the best hyperparameters\nbest_params = study.best_params\nmodel = lgb.LGBMClassifier(**best_params)\nmodel.fit(x_train_resampled, y_train_resampled)\n\n# Make predictions and evaluate the final model\ny_pred = model.predict(x_test)\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.124928Z","iopub.status.idle":"2024-09-16T14:30:24.125333Z","shell.execute_reply.started":"2024-09-16T14:30:24.125136Z","shell.execute_reply":"2024-09-16T14:30:24.125156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GAN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.126703Z","iopub.status.idle":"2024-09-16T14:30:24.127147Z","shell.execute_reply.started":"2024-09-16T14:30:24.126926Z","shell.execute_reply":"2024-09-16T14:30:24.126948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_generator(noise_dim):\n    model = tf.keras.Sequential()\n    \n#     model.add(layers.Dense(16, input_dim= noise_dim))\n#     model.add(layers.LeakyReLU(alpha=0.2))\n#     model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(32, input_dim= noise_dim),)\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(64))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization(momentum=0.8))\n    \n    #model.add(layers.Dense(128, input_dim=noise_dim))\n    model.add(layers.Dense(128))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(256))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(512))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(1024))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(np.prod(features.shape[1:]), activation='tanh'))\n    model.add(layers.Reshape(features.shape[1:]))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.129000Z","iopub.status.idle":"2024-09-16T14:30:24.129415Z","shell.execute_reply.started":"2024-09-16T14:30:24.129209Z","shell.execute_reply":"2024-09-16T14:30:24.129231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_discriminator(data_shape):\n    model= tf.keras.Sequential()\n    model.add(layers.Flatten(input_shape= data_shape))\n    model.add(layers.Dense(512))\n    model.add(layers.LeakyReLU(alpha= 0.2))\n    model.add(layers.Dense(256))\n    model.add(layers.LeakyReLU(alpha= 0.2))\n    model.add(layers.Dense(128))\n    model.add(layers.LeakyReLU(alpha= 0.2))\n    \n    model.add(layers.Dense(64))\n    model.add(layers.LeakyReLU(alpha= 0.2))\n    model.add(layers.Dense(32))\n    model.add(layers.LeakyReLU(alpha= 0.2))\n    \n    model.add(layers.Dense(1, activation= 'sigmoid'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.130839Z","iopub.status.idle":"2024-09-16T14:30:24.131271Z","shell.execute_reply.started":"2024-09-16T14:30:24.131069Z","shell.execute_reply":"2024-09-16T14:30:24.131090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise_dim= 100\ngenerator= build_generator(noise_dim)\ndiscriminator= build_discriminator(features.shape[1:])\ndiscriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# GAN model\ndiscriminator.trainable = False\ngan_input = layers.Input(shape=(noise_dim,))\ngenerated_data = generator(gan_input)\ngan_output = discriminator(generated_data)\n\ngan = tf.keras.Model(gan_input, gan_output)\ngan.compile(loss='binary_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.132507Z","iopub.status.idle":"2024-09-16T14:30:24.132965Z","shell.execute_reply.started":"2024-09-16T14:30:24.132720Z","shell.execute_reply":"2024-09-16T14:30:24.132740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_gan(gan, generator, discriminator, features, noise_dim, epochs=100, batch_size=64):\n    for epoch in range(epochs):\n        # Train the discriminator\n        idx = np.random.randint(0, features.shape[0], batch_size)\n        real_data = features.iloc[idx]  # Use .iloc for integer-based indexing\n        \n        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n        generated_data = generator.predict(noise)\n        \n        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))\n        \n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        \n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n        valid_y = np.array([1] * batch_size)\n        g_loss = gan.train_on_batch(noise, valid_y)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]:.2f}] [G loss: {g_loss}]\")\n\ntrain_gan(gan, generator, discriminator, features, noise_dim, epochs=100, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.134433Z","iopub.status.idle":"2024-09-16T14:30:24.134894Z","shell.execute_reply.started":"2024-09-16T14:30:24.134631Z","shell.execute_reply":"2024-09-16T14:30:24.134651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_gan(gan, generator, discriminator, features, noise_dim, epochs=100, batch_size=64):\n#     for epoch in range(epochs):\n#         idx = np.random.randint(0, features.shape[0], batch_size)\n#         real_data = features.iloc[idx]  \n#         noise = np.random.normal(0, 1, (batch_size, noise_dim))\n#         generated_data = generator.predict(noise)\n\n#         real_labels = np.ones((batch_size, 1))\n#         fake_labels = np.zeros((batch_size, 1))\n        \n#         d_loss_real, d_acc_real = discriminator.train_on_batch(real_data, real_labels)\n#         d_loss_fake, d_acc_fake = discriminator.train_on_batch(generated_data, fake_labels)\n        \n#         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        \n#         real_preds = discriminator.predict(real_data)\n#         fake_preds = discriminator.predict(generated_data)\n        \n#         real_preds_binary = (real_preds > 0.5).astype(int)\n#         fake_preds_binary = (fake_preds > 0.5).astype(int)\n        \n#         true_labels_real = np.ones(real_preds_binary.shape)\n#         true_labels_fake = np.zeros(fake_preds_binary.shape)\n        \n#         cm_real = confusion_matrix(true_labels_real, real_preds_binary)\n#         cm_fake = confusion_matrix(true_labels_fake, fake_preds_binary)\n        \n#         tn_real, fp_real, fn_real, tp_real = cm_real.ravel() if cm_real.size == 4 else (0, 0, 0, 0)\n#         tn_fake, fp_fake, fn_fake, tp_fake = cm_fake.ravel() if cm_fake.size == 4 else (0, 0, 0, 0)\n        \n#         total_fp = fp_real + fp_fake\n#         total_fn = fn_real + fn_fake\n#         total_tp = tp_real + tp_fake\n#         total_tn = tn_real + tn_fake\n\n#         noise = np.random.normal(0, 1, (batch_size, noise_dim))\n#         valid_y = np.array([1] * batch_size)\n#         g_loss = gan.train_on_batch(noise, valid_y)\n\n#         if epoch % 100 == 0:\n#             print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss}, acc.: {100*d_acc_real:.2f}] [G loss: {g_loss}]\")\n#             print(f\"True Positives: {total_tp}\")\n#             print(f\"False Positives: {total_fp}\")\n#             print(f\"True Negatives: {total_tn}\")\n#             print(f\"False Negatives: {total_fn}\")\n# train_gan(gan, generator, discriminator, features, noise_dim, epochs=100, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.136175Z","iopub.status.idle":"2024-09-16T14:30:24.136577Z","shell.execute_reply.started":"2024-09-16T14:30:24.136366Z","shell.execute_reply":"2024-09-16T14:30:24.136386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_generated_samples = 2800\n# Generate synthetic data\nnoise = np.random.normal(0, 1, (num_generated_samples, noise_dim))\ngenerated_data = generator.predict(noise)\n\ncombined_data = np.concatenate([features, generated_data], axis=0)\ncombined_targets = np.concatenate([target, np.ones(num_generated_samples)], axis=0)  #synthetic data targets are 1","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.138205Z","iopub.status.idle":"2024-09-16T14:30:24.138589Z","shell.execute_reply.started":"2024-09-16T14:30:24.138396Z","shell.execute_reply":"2024-09-16T14:30:24.138416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_zeros = np.sum(combined_targets == 0)\ncount_ones = np.sum(combined_targets == 1)\n\nprint(f\"Count of 0s (No Heart Attack): {count_zeros}\")\nprint(f\"Count of 1s (Heart Attack): {count_ones}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.142739Z","iopub.status.idle":"2024-09-16T14:30:24.143222Z","shell.execute_reply.started":"2024-09-16T14:30:24.143001Z","shell.execute_reply":"2024-09-16T14:30:24.143025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(combined_data, combined_targets, test_size=0.2, random_state=42)\n\nprint('Train shape:', x_train.shape)\nprint('Test shape:', x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.145230Z","iopub.status.idle":"2024-09-16T14:30:24.145673Z","shell.execute_reply.started":"2024-09-16T14:30:24.145456Z","shell.execute_reply":"2024-09-16T14:30:24.145478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import lightgbm as lgb\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# lgbm = lgb.LGBMClassifier(\n#     boosting_type='dart',\n#     learning_rate=0.05,\n#     n_estimators=40,\n#     num_leaves=31,\n#     random_state=42\n# )\n# lgbm.fit(x_train, y_train)\n\n# y_pred = lgbm.predict(x_test)\n# y_prob = lgbm.predict_proba(x_test)[:, 1]\n\n# accuracy = accuracy_score(y_test, y_pred)\n# precision = precision_score(y_test, y_pred)\n# recall = recall_score(y_test, y_pred)\n# f1 = f1_score(y_test, y_pred)\n# roc_auc = roc_auc_score(y_test, y_prob)\n\n# print(f'Accuracy: {accuracy:.4f}')\n# print(f'Precision: {precision:.4f}')\n# print(f'Recall: {recall:.4f}')\n# print(f'F1 Score: {f1:.4f}')\n# print(f'ROC AUC Score: {roc_auc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.147120Z","iopub.status.idle":"2024-09-16T14:30:24.147517Z","shell.execute_reply.started":"2024-09-16T14:30:24.147318Z","shell.execute_reply":"2024-09-16T14:30:24.147338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate_model(model, x_train, y_train, x_test, y_test):\n#     # Predictions\n#     y_pred_train = model.predict(x_train)\n#     y_pred_test = model.predict(x_test)\n    \n#     accuracy_train = accuracy_score(y_train, y_pred_train)\n#     accuracy_test = accuracy_score(y_test, y_pred_test)\n#     precision_test = precision_score(y_test, y_pred_test)\n#     recall_test = recall_score(y_test, y_pred_test)\n#     f1_test = f1_score(y_test, y_pred_test)\n#     auc = roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n#     mcc_test = matthews_corrcoef(y_test, y_pred_test)\n#     kappa_test = cohen_kappa_score(y_test, y_pred_test)\n\n#     tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n#     specificity_test = tn / (tn + fp)\n#     sensitivity_test = recall_test  \n    \n#     print(f\"Train Accuracy: {accuracy_train*100:.2f}%\")\n#     print(f\"Test Accuracy: {accuracy_test*100:.2f}%\")\n#     print(f\"Precision: {precision_test*100:.2f}%\")\n#     print(f\"Recall/Sensitivity: {sensitivity_test*100:.2f}%\")\n#     print(f\"Specificity: {specificity_test*100:.2f}%\")\n#     print(f\"F1-Score: {f1_test*100:.2f}%\")\n#     print(f\"AUC-ROC: {auc*100:.2f}%\")\n#     print(f\"MCC: {mcc_test*100:.2f}%\")\n#     print(f\"Kappa: {kappa_test*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.149068Z","iopub.status.idle":"2024-09-16T14:30:24.149462Z","shell.execute_reply.started":"2024-09-16T14:30:24.149263Z","shell.execute_reply":"2024-09-16T14:30:24.149283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"RandomForest\": RandomForestClassifier(random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n    \"ExtraTrees\": ExtraTreesClassifier(random_state=42),\n    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n    \"Bagging\": BaggingClassifier(random_state=42),\n    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n    \"LogisticRegression\": LogisticRegression(random_state=42),\n    \"GaussianNB\": GaussianNB(),\n    \"BernoulliNB\": BernoulliNB(),\n    #\"MultinomialNB\": MultinomialNB(),\n    \"KNeighbors\": KNeighborsClassifier(),\n    #\"RadiusNeighbors\": RadiusNeighborsClassifier(),\n    \"SVC\": SVC(probability=True, random_state=42),\n    \"XGBoost\": XGBClassifier(random_state=42),\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    \"MLPClassifier\": MLPClassifier(random_state=42)\n}\nresults_df = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'Specificity', 'ROC-AUC', 'Confusion Matrix'])\n\n\ndef evaluate_classifiers(classifiers, x_train, x_test, y_train, y_test):\n    global results_df\n    for name, clf in classifiers.items():\n        try:\n            clf.fit(x_train, y_train) \n            y_pred = clf.predict(x_test)  \n\n            cm = confusion_matrix(y_test, y_pred)\n            if len(cm.ravel()) == 4:  \n                tn, fp, fn, tp = cm.ravel()\n                specificity = tn / (tn + fp) \n            else:\n                specificity = None\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred, average='weighted')\n            report = classification_report(y_test, y_pred, output_dict=True)\n            precision = report['weighted avg']['precision']\n            recall = report['weighted avg']['recall']\n\n            if hasattr(clf, \"predict_proba\"): \n                y_proba = clf.predict_proba(x_test)[:, 1]\n                roc_auc = roc_auc_score(y_test, y_proba)\n            else:\n                roc_auc = None\n                \n            new_row = pd.DataFrame({\n                'Model': [name],\n                'Accuracy': [acc],\n                'F1 Score': [f1],\n                'Precision': [precision],\n                'Recall': [recall],\n                'Specificity': [specificity],\n                'ROC-AUC': [roc_auc],\n                'Confusion Matrix': [cm]\n            })\n            results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n        except (ValueError, NotFittedError) as e:\n            print(f\"Error with classifier {name}: {e}\")\n            \nevaluate_classifiers(classifiers, x_train, x_test, y_train, y_test)\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.151583Z","iopub.status.idle":"2024-09-16T14:30:24.152041Z","shell.execute_reply.started":"2024-09-16T14:30:24.151801Z","shell.execute_reply":"2024-09-16T14:30:24.151823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exc= ExtraTreesClassifier(criterion='entropy', n_estimators= 100, max_depth= 500, min_samples_split= 2, max_features='log2')\nexc.fit(x_train,y_train)\ny_pred= exc.predict(x_test)\nprint(\"classification report: \\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.153424Z","iopub.status.idle":"2024-09-16T14:30:24.153837Z","shell.execute_reply.started":"2024-09-16T14:30:24.153625Z","shell.execute_reply":"2024-09-16T14:30:24.153647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n#lr = LogisticRegression(solver='newton-cg',multi_class='multinomial') #0.8561320754716981\n#lr =LogisticRegression(solver='saga', multi_class='multinomial') #0.8561320754716981\nlr = LogisticRegression(solver='liblinear',multi_class='ovr', C=0.1)\n#lr.fit(x_train_resampled, y_train_resampled) #fit(x_train, y_train)\nlr.fit(x_train, y_train)\nprint(\"\\n\")\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# import shap\n# explainer = shap.Explainer(lr, x_train)\n# shap_values = explainer.shap_values(x_test[:100])\n# plt.figure(figsize=(16, 12))\n# shap.summary_plot(shap_values, features=x_test[:100], feature_names=features.columns)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.155380Z","iopub.status.idle":"2024-09-16T14:30:24.155813Z","shell.execute_reply.started":"2024-09-16T14:30:24.155596Z","shell.execute_reply":"2024-09-16T14:30:24.155618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\n\nsgd_clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=5000, random_state=500))\n#sgd_clf.fit(x_train_resampled, y_train_resampled)  #fit(x_train, y_train)\nsgd_clf.fit(x_train, y_train)\n\ny_pred = sgd_clf.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.157369Z","iopub.status.idle":"2024-09-16T14:30:24.157768Z","shell.execute_reply.started":"2024-09-16T14:30:24.157561Z","shell.execute_reply":"2024-09-16T14:30:24.157581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n#model=RandomForestClassifier(n_jobs=-1,random_state=42,max_features=2,min_samples_split=80, min_samples_leaf=50,bootstrap=False)\nmodel=RandomForestClassifier(class_weight='balanced')\nmodel.fit(x_train,y_train)\nmodel.fit(x_train,y_train)\nevaluate_model(model, x_train, y_train, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.159070Z","iopub.status.idle":"2024-09-16T14:30:24.159458Z","shell.execute_reply.started":"2024-09-16T14:30:24.159261Z","shell.execute_reply":"2024-09-16T14:30:24.159281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RandomForestClassifier(n_jobs=-1,random_state=42,n_estimators=5,max_depth=7,max_leaf_nodes=4,\n                             max_features=32).fit(x_train, y_train)\nevaluate_model(model, x_train, y_train, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:30:24.161214Z","iopub.status.idle":"2024-09-16T14:30:24.161604Z","shell.execute_reply.started":"2024-09-16T14:30:24.161408Z","shell.execute_reply":"2024-09-16T14:30:24.161428Z"},"trusted":true},"execution_count":null,"outputs":[]}]}